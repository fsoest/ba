% Encoding: UTF-8
@misc{bernardo2020unravelling,
      title={Unravelling the role of coherence in the first law of quantum thermodynamics},
      author={Bertúlio de Lima Bernardo},
      year={2020},
      eprint={2009.11370},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@Article{beyer2020,
  author    = {Beyer, Konstantin and Luoma, Kimmo and Strunz, Walter T.},
  title     = {Work as an external quantum observable and an operational quantum work fluctuation theorem},
  journal   = {Phys. Rev. Research},
  year      = {2020},
  volume    = {2},
  pages     = {033508},
  month     = {Sep},
  doi       = {10.1103/PhysRevResearch.2.033508},
  issue     = {3},
  numpages  = {9},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.033508},
}

@Book{Mitchell97,
  title     = {Machine Learning},
  publisher = {McGraw-Hill},
  year      = {1997},
  author    = {Mitchell, Tom M.},
  address   = {New York},
  isbn      = {978-0-07-042807-2},
  abstract  = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning---including probability and statistics, artificial intelligence, and neural networks---unifying them all in a logical and coherent manner.},
  added-at  = {2017-05-08T14:37:30.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/23e79734ee1a6e49aee02ffd108224d1c/flint63},
  file      = {eBook:1900-99/Mitchell97.pdf:PDF;McGraw-Hill Product page:http\://www.mhprofessional.com/product.php?isbn=0070428077:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0070428077/:URL},
  groups    = {public},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {3e79734ee1a6e49aee02ffd108224d1c},
  keywords  = {01624 105 book shelf ai learn algorithm},
  timestamp = {2017-07-13T17:10:10.000+0200},
  username  = {flint63},
}

@Article{LeNail2019,
  author    = {Alexander LeNail},
  title     = {NN-SVG: Publication-Ready Neural Network Architecture Schematics},
  journal   = {Journal of Open Source Software},
  year      = {2019},
  volume    = {4},
  number    = {33},
  pages     = {747},
  doi       = {10.21105/joss.00747},
  publisher = {The Open Journal},
  url       = {https://doi.org/10.21105/joss.00747},
}

@Conference{Maas2013RectifierNI,
  author        = {Maas, A.L. and Hannun, A.Y. and Ng, A.Y.},
  title         = {Rectifier Nonlinearities Improve Neural Network Acoustic Models},
  booktitle     = {Proceedings of the International Conference on Machine Learning},
  year          = {2013},
  address       = {Atlanta, Georgia},
  date-added    = {2013-12-03 21:52:00 +0000},
  date-modified = {2013-12-12 19:00:23 +0000},
}

@Book{nielsenneural,
  title     = {Neural Networks and Deep Learning},
  publisher = {Determination Press},
  year      = {2015},
  author    = {Nielsen, Michael A.},
  added-at  = {2019-01-15T22:46:49.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/274383acee84241145ff4ffede9658206/slicside},
  interhash = {04d527cadd39f888fc3babcad3343362},
  intrahash = {74383acee84241145ff4ffede9658206},
  keywords  = {ba-2018-hahnrico},
  timestamp = {2019-01-15T22:46:49.000+0100},
  type      = {misc},
  url       = {http://neuralnetworksanddeeplearning.com/},
}

@Article{rumelhart1986learning,
  author      = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  title       = {Learning representations by back-propagating errors},
  journal     = {Nature},
  year        = {1986},
  volume      = {323},
  number      = {6088},
  pages       = {533--536},
  month       = oct,
  added-at    = {2016-11-14T16:40:37.000+0100},
  biburl      = {https://www.bibsonomy.org/bibtex/201265362c8c55c577857bcb9db678267/albinzehe},
  comment     = {10.1038/323533a0},
  description = {Learning representations by back-propagating errors},
  interhash   = {c354bc293fa9aa7caffc66d40a014903},
  intrahash   = {01265362c8c55c577857bcb9db678267},
  keywords    = {deeplearning ma-zehe neuralnet},
  timestamp   = {2016-11-14T16:40:37.000+0100},
  url         = {http://dx.doi.org/10.1038/323533a0},
}

@Book{TN_libero_mab2)53517,
  title     = {Simulation neuronaler Netze},
  publisher = {Oldenbourg},
  year      = {1997},
  author    = {Zell, Andreas},
  address   = {München},
  edition   = {2., unveränd. Nachdr.},
  isbn      = {3486243500},
  abstract  = {Das Buch erschien 1994 im Addison Wesley Longman Verlag},
  keywords  = { Hochschulschrift , Neuronales Netz , Simulation },
  url       = { http://slubdd.de/katalog?TN_libero_mab2)53517 },
}

@Article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@Article{lu2020dying,
  author    = {Lu, Lu},
  title     = {Dying ReLU and Initialization: Theory and Numerical Examples},
  journal   = {Communications in Computational Physics},
  year      = {2020},
  volume    = {28},
  number    = {5},
  pages     = {1671–1706},
  month     = {Jun},
  issn      = {1991-7120},
  doi       = {10.4208/cicp.oa-2020-0165},
  publisher = {Global Science Press},
  url       = {http://dx.doi.org/10.4208/cicp.OA-2020-0165},
}

@Article{krizhevsky,
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
  title   = {ImageNet Classification with Deep Convolutional Neural Networks},
  journal = {Neural Information Processing Systems},
  year    = {2012},
  volume  = {25},
  month   = {01},
  doi     = {10.1145/3065386},
}

@InBook{LeCun2012,
  pages     = {9--48},
  title     = {Efficient BackProp},
  publisher = {Springer Berlin Heidelberg},
  year      = {2012},
  author    = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor    = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-35289-8},
  abstract  = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.},
  booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
  doi       = {10.1007/978-3-642-35289-8_3},
  url       = {https://doi.org/10.1007/978-3-642-35289-8_3},
}

@Article{Lorenzo_2017,
  author    = {Lorenzo, Salvatore and Ciccarello, Francesco and Palma, G. Massimo},
  title     = {Composite quantum collision models},
  journal   = {Physical Review A},
  year      = {2017},
  volume    = {96},
  number    = {3},
  month     = {Sep},
  issn      = {2469-9934},
  doi       = {10.1103/physreva.96.032107},
  publisher = {American Physical Society (APS)},
  url       = {http://dx.doi.org/10.1103/PhysRevA.96.032107},
}

@Article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html},
}

@InProceedings{10.1007/978-3-642-46466-9_18,
  author    = {Fukushima, Kunihiko and Miyake, Sei},
  title     = {Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition},
  booktitle = {Competition and Cooperation in Neural Nets},
  year      = {1982},
  editor    = {Amari, Shun-ichi and Arbib, Michael A.},
  pages     = {267--285},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {A neural network model, called a ``neocognitron'', is proposed for a mechanism of visual pattern recognition. It is demonstrated by computer simulation that the neocognitron has characteristics similar to those of visual systems of vertebrates.},
  isbn      = {978-3-642-46466-9},
}

@Article{doi:10.1162/neco.1997.9.8.1735,
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title    = {Long Short-Term Memory},
  journal  = {Neural Computation},
  year     = {1997},
  volume   = {9},
  number   = {8},
  pages    = {1735-1780},
  abstract = { Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms. },
  doi      = {10.1162/neco.1997.9.8.1735},
  eprint   = {https://doi.org/10.1162/neco.1997.9.8.1735},
  url      = { 
        https://doi.org/10.1162/neco.1997.9.8.1735
    
},
}

@Article{Deffner_2017,
  author    = {Deffner, Sebastian and Campbell, Steve},
  title     = {Quantum speed limits: from {Heisenberg}’s uncertainty principle to optimal quantum control},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  year      = {2017},
  volume    = {50},
  number    = {45},
  pages     = {453001},
  month     = {Oct},
  issn      = {1751-8121},
  doi       = {10.1088/1751-8121/aa86c6},
  publisher = {IOP Publishing},
  url       = {http://dx.doi.org/10.1088/1751-8121/aa86c6},
}

@Article{PhysRevA.67.052109,
  author    = {Giovannetti, Vittorio and Lloyd, Seth and Maccone, Lorenzo},
  title     = {Quantum limits to dynamical evolution},
  journal   = {Phys. Rev. A},
  year      = {2003},
  volume    = {67},
  pages     = {052109},
  month     = {May},
  doi       = {10.1103/PhysRevA.67.052109},
  issue     = {5},
  numpages  = {8},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevA.67.052109},
}

@InCollection{NEURIPS2019_9015,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@Book{10.5555/1972505,
  title     = {Quantum Computation and Quantum Information: 10th Anniversary Edition},
  publisher = {Cambridge University Press},
  year      = {2011},
  author    = {Nielsen, Michael A. and Chuang, Isaac L.},
  address   = {USA},
  edition   = {10th},
  isbn      = {1107002176},
  abstract  = {One of the most cited books in physics of all time, Quantum Computation and Quantum Information remains the best textbook in this exciting field of science. This 10th anniversary edition includes an introduction from the authors setting the work in context. This comprehensive textbook describes such remarkable effects as fast quantum algorithms, quantum teleportation, quantum cryptography and quantum error-correction. Quantum mechanics and computer science are introduced before moving on to describe what a quantum computer is, how it can be used to solve problems faster than 'classical' computers and its real-world implementation. It concludes with an in-depth treatment of quantum information. Containing a wealth of figures and exercises, this well-known textbook is ideal for courses on the subject, and will interest beginning graduate students and researchers in physics, computer science, mathematics, and electrical engineering.},
}

@Article{Mezzadri,
  author    = {Francesco Mezzadri},
  title     = {How to generate random matrices from the classical compact groups},
  journal   = {Notices of the American Mathematical Society},
  year      = {2007},
  volume    = {54},
  number    = {5},
  pages     = {592 -- 604},
  month     = may,
  issn      = {0002-9920},
  language  = {English},
  publisher = {American Mathematical Society},
}

@Misc{wandb,
  author = {Biewald, Lukas},
  title  = {Experiment Tracking with Weights and Biases},
  year   = {2020},
  url    = {https://www.wandb.com/},
}

@Article{bidirrnn,
  author  = {Schuster, Mike and Paliwal, Kuldip},
  title   = {Bidirectional recurrent neural networks},
  journal = {Signal Processing, IEEE Transactions on},
  year    = {1997},
  volume  = {45},
  pages   = {2673 - 2681},
  month   = {12},
  doi     = {10.1109/78.650093},
}

@Article{article,
  author  = {Schuster, Mike and Paliwal, Kuldip},
  title   = {Bidirectional recurrent neural networks},
  journal = {Signal Processing, IEEE Transactions on},
  year    = {1997},
  volume  = {45},
  pages   = {2673 - 2681},
  month   = {12},
  doi     = {10.1109/78.650093},
}

@Article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title         = {Attention Is All You Need},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1706.03762},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  eprint        = {1706.03762},
  timestamp     = {Mon, 13 Aug 2018 16:48:37 +0200},
  url           = {http://arxiv.org/abs/1706.03762},
}

@Article{Carleo_2019,
  author    = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborová, Lenka},
  title     = {Machine learning and the physical sciences},
  journal   = {Reviews of Modern Physics},
  year      = {2019},
  volume    = {91},
  number    = {4},
  month     = {Dec},
  issn      = {1539-0756},
  doi       = {10.1103/revmodphys.91.045002},
  publisher = {American Physical Society (APS)},
  url       = {http://dx.doi.org/10.1103/RevModPhys.91.045002},
}

@Article{wise2021using,
  author    = {Wise, David F. and Morton, John J.L. and Dhomkar, Siddharth},
  title     = {Using Deep Learning to Understand and Mitigate the Qubit Noise Environment},
  journal   = {PRX Quantum},
  year      = {2021},
  volume    = {2},
  pages     = {010316},
  month     = {Jan},
  doi       = {10.1103/PRXQuantum.2.010316},
  issue     = {1},
  numpages  = {14},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PRXQuantum.2.010316},
}

@Article{Liu2019,
  author   = {Liu, Feiyang and Zhang, Yulong and Dahlsten, Oscar and Wang, Fei},
  title    = {Intelligently chosen interventions have potential to outperform the diode bridge in power conditioning},
  journal  = {Scientific Reports},
  year     = {2019},
  volume   = {9},
  number   = {1},
  pages    = {8994},
  month    = {Jun},
  issn     = {2045-2322},
  abstract = {We probe the potential for intelligent intervention to enhance the power output of energy harvesters. We investigate general principles and a case study: a bi-resonant piezo electric harvester. We consider intelligent interventions via pre-programmed reversible energy-conserving operations. These include voltage bias flips and voltage phase shifts. These can be used to rectify voltages and to remove destructive interference. We choose the intervention type based on past data, using machine learning techniques. We find that in important parameter regimes the resulting interventions can outperform diode-based intervention, which in contrast has a fundamental minimum power dissipation bound.},
  day      = {20},
  doi      = {10.1038/s41598-019-45103-4},
  url      = {https://doi.org/10.1038/s41598-019-45103-4},
}

@Article{Banchi_2018,
  author    = {Leonardo Banchi and Edward Grant and Andrea Rocchetto and Simone Severini},
  title     = {Modelling non-markovian quantum processes with recurrent neural networks},
  journal   = {New Journal of Physics},
  year      = {2018},
  volume    = {20},
  number    = {12},
  pages     = {123030},
  month     = {Dec},
  abstract  = {Quantum systems interacting with an unknown environment are notoriously difficult to model, especially in presence of non-Markovian and non-perturbative effects. Here we introduce a neural network based approach, which has the mathematical simplicity of the Gorini–Kossakowski–Sudarshan–Lindblad master equation, but is able to model non-Markovian effects in different regimes. This is achieved by using recurrent neural networks (RNNs) for defining Lindblad operators that can keep track of memory effects. Building upon this framework, we also introduce a neural network architecture that is able to reproduce the entire quantum evolution, given an initial state. As an application we study how to train these models for quantum process tomography, showing that RNNs are accurate over different times and regimes.},
  doi       = {10.1088/1367-2630/aaf749},
  publisher = {{IOP} Publishing},
  url       = {https://doi.org/10.1088/1367-2630/aaf749},
}

@Article{hinton2012improving,
  author        = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title         = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal       = {CoRR},
  year          = {2012},
  volume        = {abs/1207.0580},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1207-0580.bib},
  eprint        = {1207.0580},
  timestamp     = {Mon, 13 Aug 2018 16:46:10 +0200},
  url           = {http://arxiv.org/abs/1207.0580},
}

@Article{PhysRevX.10.011006,
  author    = {Flurin, E. and Martin, L. S. and Hacohen-Gourgy, S. and Siddiqi, I.},
  title     = {Using a Recurrent Neural Network to Reconstruct Quantum Dynamics of a Superconducting Qubit from Physical Observations},
  journal   = {Phys. Rev. X},
  year      = {2020},
  volume    = {10},
  pages     = {011006},
  month     = {Jan},
  doi       = {10.1103/PhysRevX.10.011006},
  issue     = {1},
  numpages  = {10},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevX.10.011006},
}

@Book{Sutton1998,
  title     = {Reinforcement Learning: An Introduction},
  publisher = {The MIT Press},
  year      = {2018},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  edition   = {Second},
  added-at  = {2019-07-13T10:11:53.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  timestamp = {2019-07-13T10:11:53.000+0200},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
}

@Article{Egloff_2015,
  author    = {D Egloff and O C O Dahlsten and R Renner and V Vedral},
  title     = {A measure of majorization emerging from single-shot statistical mechanics},
  journal   = {New Journal of Physics},
  year      = {2015},
  volume    = {17},
  number    = {7},
  pages     = {073001},
  month     = {jul},
  abstract  = {The use of the von Neumann entropy in formulating the laws of thermodynamics has recently been challenged. It is associated with the average work whereas the work guaranteed to be extracted in any single run of an experiment is the more interesting quantity in general. We show that an expression that quantifies majorization determines the optimal guaranteed work. We argue it should therefore be the central quantity of statistical mechanics, rather than the von Neumann entropy. In the limit of many identical and independent subsystems (asymptotic i.i.d) the von Neumann entropy expressions are recovered but in the non-equilbrium regime the optimal guaranteed work can be radically different to the optimal average. Moreover our measure of majorization governs which evolutions can be realized via thermal interactions, whereas the non-decrease of the von Neumann entropy is not sufficiently restrictive. Our results are inspired by single-shot information theory.},
  doi       = {10.1088/1367-2630/17/7/073001},
  publisher = {{IOP} Publishing},
  url       = {https://doi.org/10.1088/1367-2630/17/7/073001},
}

@InBook{thomson_2011,
  pages      = {100–106},
  title      = {On an absolute thermometric scale founded on Carnot's theory of the motive power of hear, and calculated from Regnault's observations},
  publisher  = {Cambridge University Press},
  year       = {2011},
  author     = {Thomson, William},
  volume     = {1},
  series     = {Cambridge Library Collection - Physical Sciences},
  booktitle  = {Mathematical and Physical Papers},
  collection = {Cambridge Library Collection - Physical Sciences},
  doi        = {10.1017/CBO9780511996009.040},
  place      = {Cambridge},
}

@Article{1905AnP...322..132E,
  author  = {{Einstein}, A.},
  title   = {{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}},
  journal = {Annalen der Physik},
  year    = {1905},
  volume  = {322},
  number  = {6},
  pages   = {132-148},
  month   = jan,
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl  = {https://ui.adsabs.harvard.edu/abs/1905AnP...322..132E},
  doi     = {10.1002/andp.19053220607},
}

@InProceedings{kingma2017adam,
  author    = {Diederik P. Kingma and Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  editor    = {Yoshua Bengio and Yann LeCun},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  url       = {http://arxiv.org/abs/1412.6980},
}

@Misc{frazier2018tutorial,
  author        = {Peter I. Frazier},
  title         = {A Tutorial on Bayesian Optimization},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1807.02811},
  primaryclass  = {stat.ML},
}

@InProceedings{10.5555/2999325.2999464,
  author    = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  title     = {Practical Bayesian Optimization of Machine Learning Algorithms},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
  year      = {2012},
  series    = {NIPS'12},
  pages     = {2951–2959},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  abstract  = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  location  = {Lake Tahoe, Nevada},
  numpages  = {9},
}

@Article{PhysRevA.101.022113,
  author    = {Xu, Bao-Ming and Tu, Zhan Chun and Zou, Jian},
  title     = {Duality in quantum work},
  journal   = {Phys. Rev. A},
  year      = {2020},
  volume    = {101},
  pages     = {022113},
  month     = {Feb},
  doi       = {10.1103/PhysRevA.101.022113},
  issue     = {2},
  numpages  = {6},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevA.101.022113},
}

@InProceedings{8614252,
  author    = {S. {Siami-Namini} and N. {Tavakoli} and A. {Siami Namin}},
  title     = {A Comparison of ARIMA and LSTM in Forecasting Time Series},
  booktitle = {2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},
  year      = {2018},
  pages     = {1394-1401},
  doi       = {10.1109/ICMLA.2018.00227},
}

@Comment{jabref-meta: databaseType:bibtex;}
