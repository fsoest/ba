\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{nil}{}
\BKM@entry{id=1,dest={636861707465722E31},srcline={93}}{496E74726F64756374696F6E}
\citation{thomson_2011}
\citation{1905AnP...322..132E}
\citation{Egloff_2015}
\citation{beyer2020}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\citation{Carleo_2019}
\citation{wise2021using}
\citation{Liu2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=2,dest={636861707465722E32},srcline={96}}{4261636B67726F756E64}
\BKM@entry{id=3,dest={73656374696F6E2E322E31},srcline={97}}{436F6C6C6973696F6E206D6F64656C2064796E616D696373}
\citation{Lorenzo_2017}
\citation{beyer2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{background}{{2}{2}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Collision model dynamics}{2}{section.2.1}\protected@file@percent }
\newlabel{col_model}{{2.1}{2}{Collision model dynamics}{section.2.1}{}}
\newlabel{coll_eq}{{2.1}{2}{Collision model dynamics}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Collision model used in this work: drive and transducer are series of qubits that interact once with the system and evolve the reduced density operator $\rho _S$. The qubit configuration can be changed in intervals of $\Delta \mathrm  {T}$.\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{collmodel}{{2.1}{3}{Collision model used in this work: drive and transducer are series of qubits that interact once with the system and evolve the reduced density operator $\rho _S$. The qubit configuration can be changed in intervals of $\Delta \mathrm {T}$.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Piecewise constant implementation of drive and transducer qubits: the vertical axis represents an arbitrary parameter of the ancilla states. The qubit states are switched instantaneously and then kept constant for $\Delta \mathrm  {T}$ while $\rho _S$ evolves unitarily. The piece-wise constant drive and transducer settings lead to a piece-wise constant system Hamiltonian.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{pwc}{{2.2}{3}{Piecewise constant implementation of drive and transducer qubits: the vertical axis represents an arbitrary parameter of the ancilla states. The qubit states are switched instantaneously and then kept constant for $\Delta \mathrm {T}$ while $\rho _S$ evolves unitarily. The piece-wise constant drive and transducer settings lead to a piece-wise constant system Hamiltonian.\relax }{figure.caption.3}{}}
\citation{beyer2020}
\BKM@entry{id=4,dest={73656374696F6E2E322E32},srcline={102}}{53757065727669736564206D616368696E65206C6561726E696E67}
\citation{Mitchell97}
\BKM@entry{id=5,dest={73756273656374696F6E2E322E322E31},srcline={11}}{46756C6C792D636F6E6E65637465642066656564666F727761726420414E4E}
\citation{lu2020dying}
\citation{TN_libero_mab2)53517}
\newlabel{relham}{{2.2}{4}{Collision model dynamics}{equation.2.1.2}{}}
\newlabel{dw}{{2.3}{4}{Collision model dynamics}{equation.2.1.3}{}}
\newlabel{deltaham}{{2.4}{4}{Collision model dynamics}{equation.2.1.4}{}}
\newlabel{simpledeltaham}{{2.5}{4}{Collision model dynamics}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Supervised machine learning}{4}{section.2.2}\protected@file@percent }
\newlabel{sml}{{2.2}{4}{Supervised machine learning}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Fully-connected feedforward ANN}{4}{subsection.2.2.1}\protected@file@percent }
\citation{Maas2013RectifierNI}
\citation{krizhevsky}
\citation{LeNail2019}
\citation{LeNail2019}
\BKM@entry{id=6,dest={73756273656374696F6E2E322E322E32},srcline={31}}{4C6F6E672053686F72742D5465726D204D656D6F7279}
\citation{rumelhart1986learning}
\citation{10.1007/978-3-642-46466-9_18}
\citation{doi:10.1162/neco.1997.9.8.1735}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example fully-connected feedforward ANN with four layers, including input, output and two hidden layers \cite  {LeNail2019}.\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{nn}{{2.3}{5}{Example fully-connected feedforward ANN with four layers, including input, output and two hidden layers \cite {LeNail2019}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Long Short-Term Memory}{5}{subsection.2.2.2}\protected@file@percent }
\citation{NEURIPS2019_9015}
\citation{bidirrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example two-layer RNN with LSTM architecture. Each row of LSTM blocks has the same parameters and information is fed into the network sequentially.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{rnn}{{2.4}{6}{Example two-layer RNN with LSTM architecture. Each row of LSTM blocks has the same parameters and information is fed into the network sequentially.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualisation of a single LSTM cell. The input $x_t$ enters the cell in the bottom left and is concatenated with the output $h_{t\text  {-}1}$ of the previous cell. The combined data then enters multiple single-layer neural networks represented by orange rectangles with their respective activation functions. The forget gate $f_t$ removes data from the previous cell state $c_{t\text  {-}1}$ while $i_t$ and $g_t$ control how new data is added to the memory. $\tanh $ is applied elementwise over the cell state (yellow ellipse) to determine the cell output $h_t$ together with the output gate $o_t$.\relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{lstm}{{2.5}{7}{Visualisation of a single LSTM cell. The input $x_t$ enters the cell in the bottom left and is concatenated with the output $h_{t\text {-}1}$ of the previous cell. The combined data then enters multiple single-layer neural networks represented by orange rectangles with their respective activation functions. The forget gate $f_t$ removes data from the previous cell state $c_{t\text {-}1}$ while $i_t$ and $g_t$ control how new data is added to the memory. $\tanh $ is applied elementwise over the cell state (yellow ellipse) to determine the cell output $h_t$ together with the output gate $o_t$.\relax }{figure.caption.6}{}}
\BKM@entry{id=7,dest={73756273656374696F6E2E322E322E33},srcline={75}}{547261696E696E67205C303436204261636B70726F7061676174696F6E}
\citation{rumelhart1986learning}
\citation{nielsenneural}
\citation{kingma2017adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Training \& Backpropagation}{8}{subsection.2.2.3}\protected@file@percent }
\newlabel{trainbackprop}{{2.2.3}{8}{Training \& Backpropagation}{subsection.2.2.3}{}}
\BKM@entry{id=8,dest={636861707465722E33},srcline={105}}{576F726B206C6F77657220626F756E64}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Work lower bound}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{lower_bound}{{3}{9}{Work lower bound}{chapter.3}{}}
\BKM@entry{id=9,dest={636861707465722E34},srcline={108}}{4578706572696D656E74616C20726573756C7473}
\BKM@entry{id=10,dest={73656374696F6E2E342E31},srcline={109}}{496E666C75656E6365206F662054206F6E20576F726B204F7574707574}
\citation{2020SciPy-NMeth}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experimental results}{10}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Influence of $\Delta \mathrm  {T}$ on Work Output}{10}{section.4.1}\protected@file@percent }
\newlabel{dep_dt}{{4.1}{10}{Influence of $\Delta \mathrm {T}$ on Work Output}{section.4.1}{}}
\newlabel{single_work}{{4.1}{10}{Influence of $\Delta \mathrm {T}$ on Work Output}{equation.4.1.1}{}}
\citation{Deffner_2017}
\citation{PhysRevA.67.052109}
\BKM@entry{id=11,dest={73656374696F6E2E342E32},srcline={111}}{44617461206372656174696F6E2C20747261696E696E6720616E64206576616C756174696F6E}
\citation{Mezzadri}
\citation{LeCun2012}
\BKM@entry{id=12,dest={73756273656374696F6E2E342E322E31},srcline={23}}{43686F696365206F6620636F73742066756E6374696F6E20666F7220747261696E696E67}
\newlabel{dt_0}{{4.1a}{11}{$\rho _0 = \ket {0}\bra {0}$\relax }{figure.caption.7}{}}
\newlabel{sub@dt_0}{{a}{11}{$\rho _0 = \ket {0}\bra {0}$\relax }{figure.caption.7}{}}
\newlabel{dt_eigen}{{4.1b}{11}{$\rho _0 = \ket {+}\bra {+}$\relax }{figure.caption.7}{}}
\newlabel{sub@dt_eigen}{{b}{11}{$\rho _0 = \ket {+}\bra {+}$\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces (a) We plot the average work $\overline  {W}$ over $n = 500$ runs of random excitations divided by amount of qubit changes $N - 1$, with $\rho _0 = \ket {0}\bra {0}$, for multiple $N$. The error bars correspond to the standard deviation $\sigma _{W} = \sqrt  {\frac  {1}{n-1} \Sigma _i^n (\overline  {W} - W_i)^2}$. (b) We plot $\overline  {W}/(N-1)$ for multiple $N$ where the system state is initialised in an eigenstate of the drive Hamiltonian $H_{DS}$.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{dt_dep}{{4.1}{11}{(a) We plot the average work $\overline {W}$ over $n = 500$ runs of random excitations divided by amount of qubit changes $N - 1$, with $\rho _0 = \ket {0}\bra {0}$, for multiple $N$. The error bars correspond to the standard deviation $\sigma _{W} = \sqrt {\frac {1}{n-1} \Sigma _i^n (\overline {W} - W_i)^2}$. (b) We plot $\overline {W}/(N-1)$ for multiple $N$ where the system state is initialised in an eigenstate of the drive Hamiltonian $H_{DS}$.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Data creation, training and evaluation}{11}{section.4.2}\protected@file@percent }
\newlabel{embedding}{{4.2}{11}{Data creation, training and evaluation}{equation.4.2.2}{}}
\BKM@entry{id=13,dest={73756273656374696F6E2E342E322E32},srcline={30}}{4576616C756174696F6E206F66206E6574776F726B20706572666F726D616E6365}
\BKM@entry{id=14,dest={73656374696F6E2E342E33},srcline={113}}{4E3D323A204C6561726E696E672073696E676C65206A756D70206F7074696D616C20636F6E74726F6C2073657175656E636573}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Choice of cost function for training}{12}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Evaluation of network performance}{12}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}$N=2$: Learning single jump optimal control sequences}{12}{section.4.3}\protected@file@percent }
\newlabel{n_2_ml}{{4.3}{12}{$N=2$: Learning single jump optimal control sequences}{section.4.3}{}}
\BKM@entry{id=15,dest={73656374696F6E2E342E34},srcline={115}}{4E3D35}
\BKM@entry{id=16,dest={73756273656374696F6E2E342E342E31},srcline={116}}{54203D2035}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Efficiencies $\eta $ on the test data for models with a single hidden layer with 10 neurons trained on drive protocols with $N = 2$ and differing initial states $\rho _0$.\relax }}{13}{table.caption.8}\protected@file@percent }
\newlabel{n2efftable}{{4.1}{13}{Efficiencies $\eta $ on the test data for models with a single hidden layer with 10 neurons trained on drive protocols with $N = 2$ and differing initial states $\rho _0$.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}$N=5$}{13}{section.4.4}\protected@file@percent }
\newlabel{n5}{{4.4}{13}{$N=5$}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}$\Delta \mathrm  {T} = 5$}{13}{subsection.4.4.1}\protected@file@percent }
\newlabel{n_5_ml}{{4.4.1}{13}{$\Delta \mathrm {T} = 5$}{subsection.4.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Architecture of the LSTM networks. The green blocks represent the unidirectional case. For the bidirectional case, a second LSTM block (orange) is included with separate trainable parameters. Each LSTM block includes multiple layers of LSTM cells. The blocks labeled `FC' represent fully-connected layers.\relax }}{13}{figure.caption.9}\protected@file@percent }
\newlabel{lstm_network}{{4.2}{13}{Architecture of the LSTM networks. The green blocks represent the unidirectional case. For the bidirectional case, a second LSTM block (orange) is included with separate trainable parameters. Each LSTM block includes multiple layers of LSTM cells. The blocks labeled `FC' represent fully-connected layers.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Efficiencies $\eta $ on the test data for model architectures with given number of trainable parameters for $N=5, \ \Delta \mathrm  {T} = 5$.\relax }}{14}{table.caption.10}\protected@file@percent }
\newlabel{n5efftable}{{4.2}{14}{Efficiencies $\eta $ on the test data for model architectures with given number of trainable parameters for $N=5, \ \Delta \mathrm {T} = 5$.\relax }{table.caption.10}{}}
\newlabel{bloch_10553}{{4.3a}{15}{$W_{opt} = 3.03, W_{pred} = 1.11$\relax }{figure.caption.11}{}}
\newlabel{sub@bloch_10553}{{a}{15}{$W_{opt} = 3.03, W_{pred} = 1.11$\relax }{figure.caption.11}{}}
\newlabel{bloch_worst}{{4.3b}{15}{$W_{opt} = 3.06, W_{pred} = -2.65$\relax }{figure.caption.11}{}}
\newlabel{sub@bloch_worst}{{b}{15}{$W_{opt} = 3.06, W_{pred} = -2.65$\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {(a)} We plot the evolution of a single sample from the test set for $N=5$ and $\Delta \mathrm  {T} = 5$. For this sample we have $W_{opt} = 3.03, W_{pred} = 1.11$. Each Bloch sphere shows $\rho _S^n$ (blue dot), $\rho _S^{n+1}$ (black square), the partial system Hamiltonian acting only between system and drive $H_{DS}^n$ (red vector) as well as $H_{ST}^n$ (yellow vector) and $H_{ST}^{n+1}$ (green vector). \textbf  {Top row:} we plot the system dynamics for the transducer series generated by the optimiser for $n \in [1, N - 1]$. In the optimal case, $H_{ST}^n$ is chosen such that $\rho _S$ remains near the x-y-plane for all times and $\Tr {\rho _S^{n+1} H_{DS}^n}$ is large. \textbf  {Bottom row:} we plot the dynamics for the same drive protocol with transducer qubits predicted by the bidirectional LSTM. The overall difference of $\rho _S$ to the x-y-plane is larger. As shown in Figure \ref  {bilstmbox}, $\theta _T$ is often set to $\frac  {\pi }{2}$ which maximises the strength of $H_{ST}$ as can be seen in all Bloch spheres in the bottom row. In this case, the $H_{DS}$ are chosen by the network to be antiparallel, irrespective of the current system state. \textbf  {(b)} We show the same plot as in (a) for the worst performing sample from the test set to illustrate a shortcoming of the model. As can be seen from the yellow and green vectors, the predictions for $n \in [2, N]$ are very close to the optimal solutions (top row of (b)). However, the first transducer prediction is wrong, leading to a deviation from the optimal system dynamics.\relax }}{15}{figure.caption.11}\protected@file@percent }
\newlabel{n_5_blochs}{{4.3}{15}{\textbf {(a)} We plot the evolution of a single sample from the test set for $N=5$ and $\Delta \mathrm {T} = 5$. For this sample we have $W_{opt} = 3.03, W_{pred} = 1.11$. Each Bloch sphere shows $\rho _S^n$ (blue dot), $\rho _S^{n+1}$ (black square), the partial system Hamiltonian acting only between system and drive $H_{DS}^n$ (red vector) as well as $H_{ST}^n$ (yellow vector) and $H_{ST}^{n+1}$ (green vector). \textbf {Top row:} we plot the system dynamics for the transducer series generated by the optimiser for $n \in [1, N - 1]$. In the optimal case, $H_{ST}^n$ is chosen such that $\rho _S$ remains near the x-y-plane for all times and $\Tr {\rho _S^{n+1} H_{DS}^n}$ is large. \textbf {Bottom row:} we plot the dynamics for the same drive protocol with transducer qubits predicted by the bidirectional LSTM. The overall difference of $\rho _S$ to the x-y-plane is larger. As shown in Figure \ref {bilstmbox}, $\theta _T$ is often set to $\frac {\pi }{2}$ which maximises the strength of $H_{ST}$ as can be seen in all Bloch spheres in the bottom row. In this case, the $H_{DS}$ are chosen by the network to be antiparallel, irrespective of the current system state. \textbf {(b)} We show the same plot as in (a) for the worst performing sample from the test set to illustrate a shortcoming of the model. As can be seen from the yellow and green vectors, the predictions for $n \in [2, N]$ are very close to the optimal solutions (top row of (b)). However, the first transducer prediction is wrong, leading to a deviation from the optimal system dynamics.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces For the bidirectional LSTM network and $\Delta \mathrm  {T} = 5$, we plot boxplots of the optimal \textbf  {(a)}, predicted \textbf  {(b)} and absolute differences \textbf  {(c)} of $\theta ^n_T$ for the five qubits of each trajectory in the test set. The prediction for $\theta _T^N$ is very good as the optimal solution is to set it to $\frac  {\pi }{2}$ in all cases to maximise the work output of the final step. The prediction for $\theta _T^0$ is reasonably good too, as the optimal solution is again $\frac  {\pi }{2}$ in a majority of trajectories. The network is unable to predict the three central qubits, with median absolute differences of approximately $0.5$. \textbf  {(d)}: we plot the absolute difference between optimal and predicted $\phi _T^n$. We refrain from plotting the optimal and predicted $\phi _T^n$ themselves as these are distributed equally.\relax }}{16}{figure.caption.12}\protected@file@percent }
\newlabel{bilstmbox}{{4.4}{16}{For the bidirectional LSTM network and $\Delta \mathrm {T} = 5$, we plot boxplots of the optimal \textbf {(a)}, predicted \textbf {(b)} and absolute differences \textbf {(c)} of $\theta ^n_T$ for the five qubits of each trajectory in the test set. The prediction for $\theta _T^N$ is very good as the optimal solution is to set it to $\frac {\pi }{2}$ in all cases to maximise the work output of the final step. The prediction for $\theta _T^0$ is reasonably good too, as the optimal solution is again $\frac {\pi }{2}$ in a majority of trajectories. The network is unable to predict the three central qubits, with median absolute differences of approximately $0.5$. \textbf {(d)}: we plot the absolute difference between optimal and predicted $\phi _T^n$. We refrain from plotting the optimal and predicted $\phi _T^n$ themselves as these are distributed equally.\relax }{figure.caption.12}{}}
\BKM@entry{id=17,dest={73756273656374696F6E2E342E342E32},srcline={119}}{54203D2031}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}$\Delta \mathrm  {T} = 1$}{17}{subsection.4.4.2}\protected@file@percent }
\newlabel{n_5_dt1}{{4.4.2}{17}{$\Delta \mathrm {T} = 1$}{subsection.4.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Efficiencies $\eta $ and MSE loss on the test set for differing model architectures for $N=5, \Delta \mathrm  {T} = 1$.\relax }}{17}{table.caption.13}\protected@file@percent }
\newlabel{effdt1}{{4.3}{17}{Efficiencies $\eta $ and MSE loss on the test set for differing model architectures for $N=5, \Delta \mathrm {T} = 1$.\relax }{table.caption.13}{}}
\newlabel{}{{4.5a}{18}{$\Delta \mathrm {T} = 1: W_{opt} = 1.40, W_{pred} = 1.32$\relax }{figure.caption.14}{}}
\newlabel{sub@}{{a}{18}{$\Delta \mathrm {T} = 1: W_{opt} = 1.40, W_{pred} = 1.32$\relax }{figure.caption.14}{}}
\newlabel{}{{4.5b}{18}{$\Delta \mathrm {T} = 5: W_{opt} = 2.42, W_{pred} = 0.57$\relax }{figure.caption.14}{}}
\newlabel{sub@}{{b}{18}{$\Delta \mathrm {T} = 5: W_{opt} = 2.42, W_{pred} = 0.57$\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces We plot the optimal (top row) and predicted (bottom row) system state and transducer settings for the bidirectional LSTM, using the same representation as in Figure \ref  {n_5_blochs}, for $\Delta \mathrm  {T} = 1$ (a) and $\Delta \mathrm  {T} = 5$ (b). The drive sequence is the same in all rows and is a sample from the test set. For $\Delta \mathrm  {T} = 1$, the optimal system state, where $\Tr {\rho _S^i \sigma _z} = 0$, cannot be reached. However, the discrepancy of the system states (blue/black dots) between optimal (top row) and predicted (bottom row) policies are smaller than for $\Delta \mathrm  {T} = 5$.\relax }}{18}{figure.caption.14}\protected@file@percent }
\newlabel{blochsdt15}{{4.5}{18}{We plot the optimal (top row) and predicted (bottom row) system state and transducer settings for the bidirectional LSTM, using the same representation as in Figure \ref {n_5_blochs}, for $\Delta \mathrm {T} = 1$ (a) and $\Delta \mathrm {T} = 5$ (b). The drive sequence is the same in all rows and is a sample from the test set. For $\Delta \mathrm {T} = 1$, the optimal system state, where $\Tr {\rho _S^i \sigma _z} = 0$, cannot be reached. However, the discrepancy of the system states (blue/black dots) between optimal (top row) and predicted (bottom row) policies are smaller than for $\Delta \mathrm {T} = 5$.\relax }{figure.caption.14}{}}
\BKM@entry{id=18,dest={73756273656374696F6E2E342E342E33},srcline={122}}{4E6F69736520726573697374616E6365}
\citation{10.5555/1972505}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces We plot the performance of the bidirectional LSTM for $\Delta \mathrm  {T} = 1$. Unlike the optimum for the longer switching time, $\theta _T^n = \frac  {\pi }{2}$ for a majority of the central three qubits as well. Again, predicted and optimal $\phi ^n_T$ are distributed equally and therefore we do not plot them here.\relax }}{19}{figure.caption.15}\protected@file@percent }
\newlabel{dt1box}{{4.6}{19}{We plot the performance of the bidirectional LSTM for $\Delta \mathrm {T} = 1$. Unlike the optimum for the longer switching time, $\theta _T^n = \frac {\pi }{2}$ for a majority of the central three qubits as well. Again, predicted and optimal $\phi ^n_T$ are distributed equally and therefore we do not plot them here.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Noise resistance}{19}{subsection.4.4.3}\protected@file@percent }
\newlabel{ml_noise}{{4.4.3}{19}{Noise resistance}{subsection.4.4.3}{}}
\BKM@entry{id=19,dest={73756273656374696F6E2E342E342E34},srcline={125}}{45787472616374656420776F726B20617320636F73742066756E6374696F6E}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces We plot the difference $\Delta W = \overline  {W}_{noise} - W_{pred}$ of each element of the test set for the bidirectional LSTM. $W_{pred}$ is the work output following the model prediction. \textbf  {(a), (b)}: we create 100 noisy drive sequences and calculate the average $\overline  {W}_{noise}$ of their work output following the predicted transducer protocol. \textbf  {(c), (d)}: we create 100 noisy transducer sequences and calculate the average of their work output with a given drive sequence from the test set. In both plots, the black dots indicate the average fidelities and $\Delta W$ for a given $\tau $.\relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{noisedt5}{{4.7}{20}{We plot the difference $\Delta W = \overline {W}_{noise} - W_{pred}$ of each element of the test set for the bidirectional LSTM. $W_{pred}$ is the work output following the model prediction. \textbf {(a), (b)}: we create 100 noisy drive sequences and calculate the average $\overline {W}_{noise}$ of their work output following the predicted transducer protocol. \textbf {(c), (d)}: we create 100 noisy transducer sequences and calculate the average of their work output with a given drive sequence from the test set. In both plots, the black dots indicate the average fidelities and $\Delta W$ for a given $\tau $.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Extracted work as cost function}{20}{subsection.4.4.4}\protected@file@percent }
\newlabel{work_cost}{{4.4.4}{20}{Extracted work as cost function}{subsection.4.4.4}{}}
\BKM@entry{id=20,dest={73656374696F6E2E342E35},srcline={128}}{47656E6572616C69736174696F6E20746F206F74686572204E}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Generalisation to other $N$}{21}{section.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Generalisability of MSE and local optimisation models. For each $N$, we generate 1000 random drives and predict their transducer policies. We plot the mean of their work output $\overline  {W}$ for varying $N$. Where available, we plot the optimal average work output. For $\Delta \mathrm  {T} = 1$, we additionally plot $\overline  {W}$ for the bidirectional LSTM trained using the extracted work as the loss function. The average output of the local optimisation protocol is plotted as a benchmark.\relax }}{21}{figure.caption.17}\protected@file@percent }
\newlabel{genplot}{{4.8}{21}{Generalisability of MSE and local optimisation models. For each $N$, we generate 1000 random drives and predict their transducer policies. We plot the mean of their work output $\overline {W}$ for varying $N$. Where available, we plot the optimal average work output. For $\Delta \mathrm {T} = 1$, we additionally plot $\overline {W}$ for the bidirectional LSTM trained using the extracted work as the loss function. The average output of the local optimisation protocol is plotted as a benchmark.\relax }{figure.caption.17}{}}
\BKM@entry{id=21,dest={636861707465722E35},srcline={131}}{53756D6D61727920616E64206F75746C6F6F6B}
\citation{Banchi_2018}
\citation{PhysRevX.10.011006}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary and outlook}{23}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{outlook}{{5}{23}{Summary and outlook}{chapter.5}{}}
\citation{Sutton1998}
\bibstyle{unsrt}
\bibdata{biblio}
\BKM@entry{id=22,dest={636861707465722E36},srcline={1}}{4269626C696F677261706879}
\bibcite{thomson_2011}{1}
\bibcite{1905AnP...322..132E}{2}
\bibcite{Egloff_2015}{3}
\bibcite{beyer2020}{4}
\bibcite{DBLP:journals/corr/VaswaniSPUJGKP17}{5}
\bibcite{Carleo_2019}{6}
\bibcite{wise2021using}{7}
\bibcite{Liu2019}{8}
\bibcite{Lorenzo_2017}{9}
\bibcite{Mitchell97}{10}
\bibcite{lu2020dying}{11}
\bibcite{TN_libero_mab2)53517}{12}
\bibcite{Maas2013RectifierNI}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bibliography}{25}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{krizhevsky}{14}
\bibcite{LeNail2019}{15}
\bibcite{rumelhart1986learning}{16}
\bibcite{10.1007/978-3-642-46466-9_18}{17}
\bibcite{doi:10.1162/neco.1997.9.8.1735}{18}
\bibcite{NEURIPS2019_9015}{19}
\bibcite{bidirrnn}{20}
\bibcite{nielsenneural}{21}
\bibcite{kingma2017adam}{22}
\bibcite{2020SciPy-NMeth}{23}
\bibcite{Deffner_2017}{24}
\bibcite{PhysRevA.67.052109}{25}
\bibcite{Mezzadri}{26}
\bibcite{LeCun2012}{27}
\bibcite{10.5555/1972505}{28}
\bibcite{Banchi_2018}{29}
\bibcite{PhysRevX.10.011006}{30}
\bibcite{Sutton1998}{31}
\bibcite{hinton2012improving}{32}
\bibcite{frazier2018tutorial}{33}
\bibcite{wandb}{34}
\BKM@entry{id=23,dest={617070656E6469782E41},srcline={138}}{44657269766174696F6E73}
\BKM@entry{id=24,dest={73656374696F6E2E412E31},srcline={139}}{53696E676C65206A756D7020776F726B206F7574707574}
\BKM@entry{id=25,dest={73656374696F6E2E412E32},srcline={142}}{4F7074696D616C20706F6C69637920666F72204E3D32}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Derivations}{28}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Single jump work output}{28}{section.A.1}\protected@file@percent }
\newlabel{dwformula}{{A.1}{28}{Single jump work output}{equation.A.1.1}{}}
\newlabel{deriv_jump}{{A.1}{28}{Single jump work output}{equation.A.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Optimal policy for $N=2$}{29}{section.A.2}\protected@file@percent }
\newlabel{n2_opt_pol}{{A.2}{29}{Optimal policy for $N=2$}{section.A.2}{}}
\newlabel{commutator}{{A.2}{29}{Optimal policy for $N=2$}{equation.A.2.2}{}}
\newlabel{phiopt}{{A.3}{29}{Optimal policy for $N=2$}{equation.A.2.3}{}}
\newlabel{singleopt}{{A.5}{29}{Optimal policy for $N=2$}{equation.A.2.5}{}}
\BKM@entry{id=26,dest={617070656E6469782E42},srcline={144}}{547261696E696E672070726F746F636F6C73}
\citation{NEURIPS2019_9015}
\citation{hinton2012improving}
\BKM@entry{id=27,dest={73656374696F6E2E422E31},srcline={17}}{4879706572706172616D65746572732053656374696F6E20342E33}
\citation{kingma2017adam}
\BKM@entry{id=28,dest={73656374696F6E2E422E32},srcline={36}}{4879706572706172616D65746572732053656374696F6E20342E34}
\citation{frazier2018tutorial}
\citation{wandb}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Training protocols}{30}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{training}{{B}{30}{Training protocols}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Hyperparameters Section \ref  {n_2_ml}}{30}{section.B.1}\protected@file@percent }
\newlabel{eds}{{B.1}{30}{Hyperparameters Section \ref {n_2_ml}}{equation.B.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Hyperparameters used in training for the models in section \ref  {n_2_ml}.\relax }}{30}{table.caption.18}\protected@file@percent }
\newlabel{hyperparams_n_2}{{B.1}{30}{Hyperparameters used in training for the models in section \ref {n_2_ml}.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Hyperparameters Section \ref  {n5}}{30}{section.B.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Hyperparameters used for the models in Sections \ref  {n_5_ml} to \ref  {ml_noise}.\relax }}{31}{table.caption.19}\protected@file@percent }
\newlabel{hyperparams_n_5}{{B.2}{31}{Hyperparameters used for the models in Sections \ref {n_5_ml} to \ref {ml_noise}.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Hyperparameters used for the models in Section \ref  {work_cost}.\relax }}{31}{table.caption.20}\protected@file@percent }
\newlabel{hyperparams_work_cost}{{B.3}{31}{Hyperparameters used for the models in Section \ref {work_cost}.\relax }{table.caption.20}{}}
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{18.10382pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{25.90468pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{32.1087pt}
