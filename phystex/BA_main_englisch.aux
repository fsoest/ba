\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{nil}{}
\BKM@entry{id=1,dest={636861707465722E31},srcline={85}}{496E74726F64756374696F6E}
\citation{thomson_2011}
\citation{1905AnP...322..132E}
\citation{Egloff_2015}
\citation{PhysRevE.93.022131}
\citation{beyer2020}
\citation{WEI20171}
\citation{expharv}
\citation{sothmann}
\citation{Liu2019}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\citation{Carleo_2019}
\citation{wise2021using}
\citation{8614252}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=2,dest={636861707465722E32},srcline={88}}{4261636B67726F756E64}
\BKM@entry{id=3,dest={73656374696F6E2E322E31},srcline={89}}{436F6C6C6973696F6E206D6F64656C2064796E616D696373}
\citation{Lorenzo_2017}
\citation{beyer2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{background}{{2}{3}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Collision model dynamics}{3}{section.2.1}\protected@file@percent }
\newlabel{col_model}{{2.1}{3}{Collision model dynamics}{section.2.1}{}}
\newlabel{coll_eq}{{2.1}{3}{Collision model dynamics}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Collision model used in this work: drive and transducer are series of qubits that interact once with the system and evolve the reduced density operator $\rho _S$. The qubit configuration can be changed in intervals of $\Delta \mathrm  {T}$.\relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{collmodel}{{2.1}{4}{Collision model used in this work: drive and transducer are series of qubits that interact once with the system and evolve the reduced density operator $\rho _S$. The qubit configuration can be changed in intervals of $\Delta \mathrm {T}$.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Piecewise constant implementation of drive and transducer qubits: the vertical axis represents an arbitrary parameter of the ancilla states. The qubit states are switched instantaneously and then kept constant for $\Delta \mathrm  {T}$ while $\rho _S$ evolves unitarily. The piece-wise constant drive and transducer settings lead to a piece-wise constant system Hamiltonian.\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{pwc}{{2.2}{4}{Piecewise constant implementation of drive and transducer qubits: the vertical axis represents an arbitrary parameter of the ancilla states. The qubit states are switched instantaneously and then kept constant for $\Delta \mathrm {T}$ while $\rho _S$ evolves unitarily. The piece-wise constant drive and transducer settings lead to a piece-wise constant system Hamiltonian.\relax }{figure.caption.3}{}}
\citation{beyer2020}
\BKM@entry{id=4,dest={73656374696F6E2E322E32},srcline={94}}{53757065727669736564206D616368696E65206C6561726E696E67}
\citation{Mitchell97}
\newlabel{relham}{{2.2}{5}{Collision model dynamics}{equation.2.1.2}{}}
\newlabel{dw}{{2.3}{5}{Collision model dynamics}{equation.2.1.3}{}}
\newlabel{deltaham}{{2.4}{5}{Collision model dynamics}{equation.2.1.4}{}}
\newlabel{simpledeltaham}{{2.5}{5}{Collision model dynamics}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Supervised machine learning}{5}{section.2.2}\protected@file@percent }
\newlabel{sml}{{2.2}{5}{Supervised machine learning}{section.2.2}{}}
\BKM@entry{id=5,dest={73756273656374696F6E2E322E322E31},srcline={11}}{46756C6C792D636F6E6E65637465642066656564666F727761726420414E4E}
\citation{lu2020dying}
\citation{TN_libero_mab2)53517}
\citation{Maas2013RectifierNI}
\citation{krizhevsky}
\citation{LeNail2019}
\citation{LeNail2019}
\BKM@entry{id=6,dest={73756273656374696F6E2E322E322E32},srcline={31}}{4C6F6E672053686F72742D5465726D204D656D6F7279}
\citation{10.1007/978-3-642-46466-9_18}
\citation{8614252}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Fully-connected feedforward ANN}{6}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of a fully-connected feedforward ANN with four layers, including input, output and two hidden layers \cite  {LeNail2019}.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{nn}{{2.3}{6}{Example of a fully-connected feedforward ANN with four layers, including input, output and two hidden layers \cite {LeNail2019}.\relax }{figure.caption.4}{}}
\citation{doi:10.1162/neco.1997.9.8.1735}
\citation{NEURIPS2019_9015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Long Short-Term Memory}{7}{subsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of a two-layer RNN with LSTM architecture. Each row of LSTM blocks has the same parameters and information is fed into the network sequentially.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{rnn}{{2.4}{7}{Example of a two-layer RNN with LSTM architecture. Each row of LSTM blocks has the same parameters and information is fed into the network sequentially.\relax }{figure.caption.5}{}}
\citation{bidirrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualisation of a single LSTM cell. The input $x_t$ enters the cell in the bottom left and is concatenated with the output $h_{t\text  {-}1}$ of the previous cell. The combined data then enters multiple single-layer neural networks represented by orange rectangles with their respective activation functions. The forget gate $f_t$ removes data from the previous cell state $c_{t\text  {-}1}$ while $i_t$ and $g_t$ control how new data is added to the memory. $\tanh $ is applied elementwise over the cell state (yellow ellipse) to determine the cell output $h_t$ together with the output gate $o_t$.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{lstm}{{2.5}{8}{Visualisation of a single LSTM cell. The input $x_t$ enters the cell in the bottom left and is concatenated with the output $h_{t\text {-}1}$ of the previous cell. The combined data then enters multiple single-layer neural networks represented by orange rectangles with their respective activation functions. The forget gate $f_t$ removes data from the previous cell state $c_{t\text {-}1}$ while $i_t$ and $g_t$ control how new data is added to the memory. $\tanh $ is applied elementwise over the cell state (yellow ellipse) to determine the cell output $h_t$ together with the output gate $o_t$.\relax }{figure.caption.6}{}}
\BKM@entry{id=7,dest={73756273656374696F6E2E322E322E33},srcline={76}}{547261696E696E67205C303436204261636B70726F7061676174696F6E}
\citation{rumelhart1986learning}
\citation{nielsenneural}
\citation{kingma2017adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Training \& Backpropagation}{9}{subsection.2.2.3}\protected@file@percent }
\newlabel{trainbackprop}{{2.2.3}{9}{Training \& Backpropagation}{subsection.2.2.3}{}}
\BKM@entry{id=8,dest={636861707465722E33},srcline={97}}{576F726B206C6F77657220626F756E64}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Work lower bound}{10}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{lower_bound}{{3}{10}{Work lower bound}{chapter.3}{}}
\newlabel{dWn}{{3.1}{10}{Work lower bound}{equation.3.0.1}{}}
\BKM@entry{id=9,dest={636861707465722E34},srcline={100}}{53696D756C6174696F6E20726573756C7473}
\BKM@entry{id=10,dest={73656374696F6E2E342E31},srcline={101}}{496E666C75656E6365206F662054206F6E20576F726B204F7574707574}
\citation{2020SciPy-NMeth}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Simulation results}{11}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Influence of $\Delta \mathrm  {T}$ on Work Output}{11}{section.4.1}\protected@file@percent }
\newlabel{dep_dt}{{4.1}{11}{Influence of $\Delta \mathrm {T}$ on Work Output}{section.4.1}{}}
\newlabel{single_work}{{4.1}{11}{Influence of $\Delta \mathrm {T}$ on Work Output}{equation.4.1.1}{}}
\citation{Deffner_2017}
\citation{PhysRevA.67.052109}
\BKM@entry{id=11,dest={73656374696F6E2E342E32},srcline={103}}{44617461206372656174696F6E2C20747261696E696E6720616E64206576616C756174696F6E}
\citation{Mezzadri}
\citation{LeCun2012}
\BKM@entry{id=12,dest={73756273656374696F6E2E342E322E31},srcline={23}}{43686F696365206F6620636F73742066756E6374696F6E20666F7220747261696E696E67}
\newlabel{dt_0}{{4.1a}{12}{$\rho _0 = \ket {0}\bra {0}$\relax }{figure.caption.7}{}}
\newlabel{sub@dt_0}{{a}{12}{$\rho _0 = \ket {0}\bra {0}$\relax }{figure.caption.7}{}}
\newlabel{dt_eigen}{{4.1b}{12}{$\rho _0 = \ket {+}\bra {+}$\relax }{figure.caption.7}{}}
\newlabel{sub@dt_eigen}{{b}{12}{$\rho _0 = \ket {+}\bra {+}$\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces (a) We plot the average work $\overline  {W}$ over $n = 500$ runs of random excitations divided by the amount of qubit changes $N - 1$, with $\rho _0 = \ket {0}\bra {0}$, for multiple $N$. The error bars correspond to the standard deviation $\sigma _{W} = \sqrt  {\frac  {1}{n-1} \Sigma _i^n (\overline  {W} - W_i)^2}$. (b) We plot $\overline  {W}/(N-1)$ for multiple $N$ where the system state is initialised in an eigenstate of the drive Hamiltonian $H_{(D)S}$.\relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{dt_dep}{{4.1}{12}{(a) We plot the average work $\overline {W}$ over $n = 500$ runs of random excitations divided by the amount of qubit changes $N - 1$, with $\rho _0 = \ket {0}\bra {0}$, for multiple $N$. The error bars correspond to the standard deviation $\sigma _{W} = \sqrt {\frac {1}{n-1} \Sigma _i^n (\overline {W} - W_i)^2}$. (b) We plot $\overline {W}/(N-1)$ for multiple $N$ where the system state is initialised in an eigenstate of the drive Hamiltonian $H_{(D)S}$.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Data creation, training and evaluation}{12}{section.4.2}\protected@file@percent }
\newlabel{embedding}{{4.2}{12}{Data creation, training and evaluation}{equation.4.2.2}{}}
\BKM@entry{id=13,dest={73756273656374696F6E2E342E322E32},srcline={30}}{4576616C756174696F6E206F66206E6574776F726B20706572666F726D616E6365}
\BKM@entry{id=14,dest={73656374696F6E2E342E33},srcline={105}}{416D6F756E74206F66207175626974207377697463686573204E3D323A204C6561726E696E672073696E676C652065787472616374696F6E2073746570206F7074696D616C20636F6E74726F6C2073657175656E636573}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Choice of cost function for training}{13}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Evaluation of network performance}{13}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Amount of qubit switches $N=2$: Learning single extraction step optimal control sequences}{13}{section.4.3}\protected@file@percent }
\newlabel{n_2_ml}{{4.3}{13}{Amount of qubit switches $N=2$: Learning single extraction step optimal control sequences}{section.4.3}{}}
\BKM@entry{id=15,dest={73656374696F6E2E342E34},srcline={107}}{416D6F756E74206F66207175626974207377697463686573204E3D35}
\BKM@entry{id=16,dest={73756273656374696F6E2E342E342E31},srcline={108}}{496E746572616374696F6E2074696D652054203D2035}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Efficiencies $\eta $ on the test data for models with a single hidden layer with 10 neurons trained on drive protocols with $N = 2$ and differing initial states $\rho _0$.\relax }}{14}{table.caption.8}\protected@file@percent }
\newlabel{n2efftable}{{4.1}{14}{Efficiencies $\eta $ on the test data for models with a single hidden layer with 10 neurons trained on drive protocols with $N = 2$ and differing initial states $\rho _0$.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Amount of qubit switches $N=5$}{14}{section.4.4}\protected@file@percent }
\newlabel{n5}{{4.4}{14}{Amount of qubit switches $N=5$}{section.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Interaction time $\Delta \mathrm  {T} = 5$}{14}{subsection.4.4.1}\protected@file@percent }
\newlabel{n_5_ml}{{4.4.1}{14}{Interaction time $\Delta \mathrm {T} = 5$}{subsection.4.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Architecture of the LSTM networks. The green blocks represent the unidirectional case. For the bidirectional case, a second LSTM block (orange) is included with separate trainable parameters. Each LSTM block includes multiple layers of LSTM cells. The blocks labeled `FC' represent fully-connected layers.\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{lstm_network}{{4.2}{14}{Architecture of the LSTM networks. The green blocks represent the unidirectional case. For the bidirectional case, a second LSTM block (orange) is included with separate trainable parameters. Each LSTM block includes multiple layers of LSTM cells. The blocks labeled `FC' represent fully-connected layers.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Efficiencies $\eta $, MSE loss and average work output $W$ on the test data for model architectures with given number of trainable parameters for $N=5, \ \Delta \mathrm  {T} = 5$.\relax }}{15}{table.caption.10}\protected@file@percent }
\newlabel{n5efftable}{{4.2}{15}{Efficiencies $\eta $, MSE loss and average work output $W$ on the test data for model architectures with given number of trainable parameters for $N=5, \ \Delta \mathrm {T} = 5$.\relax }{table.caption.10}{}}
\newlabel{bloch_10553}{{4.3a}{16}{$W_{opt} = 3.03, W_{pred} = 1.11$\relax }{figure.caption.11}{}}
\newlabel{sub@bloch_10553}{{a}{16}{$W_{opt} = 3.03, W_{pred} = 1.11$\relax }{figure.caption.11}{}}
\newlabel{bloch_worst}{{4.3b}{16}{$W_{opt} = 3.06, W_{pred} = -2.65$\relax }{figure.caption.11}{}}
\newlabel{sub@bloch_worst}{{b}{16}{$W_{opt} = 3.06, W_{pred} = -2.65$\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {(a)} We plot the evolution of a single sample from the test set for $N=5$ and $\Delta \mathrm  {T} = 5$. Each Bloch sphere shows $\rho _S^n$ (blue dot), $\rho _S^{n+1}$ (black square), the partial system Hamiltonian acting only between system and drive $H_{(D)S}^n$ (red vector) as well as $H_{S(T)}^n$ (yellow vector) and $H_{S(T)}^{n+1}$ (green vector). The Hamiltonian Bloch vectors are given by $\mathaccentV {vec}17E{r}_H = \Tr {H \mathaccentV {vec}17E{\sigma }}$. \textbf  {Top row:} we plot the system dynamics for the transducer series generated by the optimiser for $n \in [1, N - 1]$. In the optimal case, $H_{S(T)}^n$ is chosen such that $\rho _S$ remains near the x-y-plane when the work is extracted and $\Tr {\rho _S^{n+1} H_{(D)S}^n}$ is large. \textbf  {Bottom row:} we plot the dynamics for the same drive protocol with transducer qubits predicted by the bidirectional LSTM. The distance of $\rho _S$ to the x-y-plane is larger. As shown in Figure \ref  {bilstmbox}, $\theta _T$ is often set to $\frac  {\pi }{2}$ which maximises the strength of $H_{S(T)}$ as can be seen in all Bloch spheres in the bottom row. In this case, the $H_{(D)S}$ are chosen by the network to be antiparallel, irrespective of the current system state. \textbf  {(b)} We show the same plot as in (a) for the worst performing sample from the test set to illustrate a shortcoming of the model. As can be seen from the yellow and green vectors, the predictions for $n \in [2, N]$ are very close to the optimal solutions (top row of (b)). However, the first transducer prediction is wrong, leading to a deviation from the optimal system dynamics.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{n_5_blochs}{{4.3}{16}{\textbf {(a)} We plot the evolution of a single sample from the test set for $N=5$ and $\Delta \mathrm {T} = 5$. Each Bloch sphere shows $\rho _S^n$ (blue dot), $\rho _S^{n+1}$ (black square), the partial system Hamiltonian acting only between system and drive $H_{(D)S}^n$ (red vector) as well as $H_{S(T)}^n$ (yellow vector) and $H_{S(T)}^{n+1}$ (green vector). The Hamiltonian Bloch vectors are given by $\vec {r}_H = \Tr {H \vec {\sigma }}$. \textbf {Top row:} we plot the system dynamics for the transducer series generated by the optimiser for $n \in [1, N - 1]$. In the optimal case, $H_{S(T)}^n$ is chosen such that $\rho _S$ remains near the x-y-plane when the work is extracted and $\Tr {\rho _S^{n+1} H_{(D)S}^n}$ is large. \textbf {Bottom row:} we plot the dynamics for the same drive protocol with transducer qubits predicted by the bidirectional LSTM. The distance of $\rho _S$ to the x-y-plane is larger. As shown in Figure \ref {bilstmbox}, $\theta _T$ is often set to $\frac {\pi }{2}$ which maximises the strength of $H_{S(T)}$ as can be seen in all Bloch spheres in the bottom row. In this case, the $H_{(D)S}$ are chosen by the network to be antiparallel, irrespective of the current system state. \textbf {(b)} We show the same plot as in (a) for the worst performing sample from the test set to illustrate a shortcoming of the model. As can be seen from the yellow and green vectors, the predictions for $n \in [2, N]$ are very close to the optimal solutions (top row of (b)). However, the first transducer prediction is wrong, leading to a deviation from the optimal system dynamics.\relax }{figure.caption.11}{}}
\BKM@entry{id=17,dest={73756273656374696F6E2E342E342E32},srcline={111}}{496E746572616374696F6E2074696D652054203D2031}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Interaction time $\Delta \mathrm  {T} = 1$}{17}{subsection.4.4.2}\protected@file@percent }
\newlabel{n_5_dt1}{{4.4.2}{17}{Interaction time $\Delta \mathrm {T} = 1$}{subsection.4.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Efficiencies $\eta $, MSE loss and average work output $W$ on the test set for differing model architectures for $N=5, \Delta \mathrm  {T} = 1$.\relax }}{17}{table.caption.13}\protected@file@percent }
\newlabel{effdt1}{{4.3}{17}{Efficiencies $\eta $, MSE loss and average work output $W$ on the test set for differing model architectures for $N=5, \Delta \mathrm {T} = 1$.\relax }{table.caption.13}{}}
\newlabel{}{{4.5a}{18}{$\Delta \mathrm {T} = 1: W_{opt} = 1.40, W_{pred} = 1.32$\relax }{figure.caption.14}{}}
\newlabel{sub@}{{a}{18}{$\Delta \mathrm {T} = 1: W_{opt} = 1.40, W_{pred} = 1.32$\relax }{figure.caption.14}{}}
\newlabel{}{{4.5b}{18}{$\Delta \mathrm {T} = 5: W_{opt} = 2.42, W_{pred} = 0.57$\relax }{figure.caption.14}{}}
\newlabel{sub@}{{b}{18}{$\Delta \mathrm {T} = 5: W_{opt} = 2.42, W_{pred} = 0.57$\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces We plot the optimal (top row) and predicted (bottom row) system state and transducer settings for the bidirectional LSTM, using the same representation as in Figure \ref  {n_5_blochs}, for $\Delta \mathrm  {T} = 1$ (a) and $\Delta \mathrm  {T} = 5$ (b). The drive sequence is the same in all rows and is a sample from the test set. For $\Delta \mathrm  {T} = 1$, the optimal system state, where $\Tr {\rho _S(n \Delta \mathrm  {T}) \sigma _z} = 0$, cannot be reached. The discrepancy of the system states (blue/black dots) between optimal (top row) and predicted (bottom row) policies are smaller than for $\Delta \mathrm  {T} = 5$.\relax }}{18}{figure.caption.14}\protected@file@percent }
\newlabel{blochsdt15}{{4.5}{18}{We plot the optimal (top row) and predicted (bottom row) system state and transducer settings for the bidirectional LSTM, using the same representation as in Figure \ref {n_5_blochs}, for $\Delta \mathrm {T} = 1$ (a) and $\Delta \mathrm {T} = 5$ (b). The drive sequence is the same in all rows and is a sample from the test set. For $\Delta \mathrm {T} = 1$, the optimal system state, where $\Tr {\rho _S(n \Delta \mathrm {T}) \sigma _z} = 0$, cannot be reached. The discrepancy of the system states (blue/black dots) between optimal (top row) and predicted (bottom row) policies are smaller than for $\Delta \mathrm {T} = 5$.\relax }{figure.caption.14}{}}
\BKM@entry{id=18,dest={73756273656374696F6E2E342E342E33},srcline={114}}{4E6F69736520726573697374616E6365}
\citation{10.5555/1972505}
\BKM@entry{id=19,dest={73756273656374696F6E2E342E342E34},srcline={117}}{45787472616374656420776F726B20617320636F73742066756E6374696F6E}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Noise resistance}{19}{subsection.4.4.3}\protected@file@percent }
\newlabel{ml_noise}{{4.4.3}{19}{Noise resistance}{subsection.4.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Extracted work as cost function}{19}{subsection.4.4.4}\protected@file@percent }
\newlabel{work_cost}{{4.4.4}{19}{Extracted work as cost function}{subsection.4.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces For the bidirectional LSTM network and $\Delta \mathrm  {T} = 5$, we plot boxplots (inside the box are values from the lower to the upper quartile (25th to 75th percentile), the orange line represents the median and the vertical lines represent the values that lie in 1.5 times the interquartile range (height of the box)) of the optimal \textbf  {(a)}, predicted \textbf  {(b)} and absolute differences \textbf  {(c)} of $\theta ^n_T$ for the five qubits of each trajectory in the test set. The prediction for $\theta _T^N$ is very good as the optimal solution is to set it to $\frac  {\pi }{2}$ in all cases to maximise the work output of the final step. The prediction for $\theta _T^0$ is reasonably good too, as the optimal solution is again $\frac  {\pi }{2}$ for a majority of trajectories. The network is unable to predict the three central qubits, with median absolute differences of approximately $0.5$. \textbf  {(d)}: we plot the absolute difference between optimal and predicted $\phi _T^n$. We refrain from plotting the optimal and predicted $\phi _T^n$ themselves as these are distributed uniformly.\relax }}{20}{figure.caption.12}\protected@file@percent }
\newlabel{bilstmbox}{{4.4}{20}{For the bidirectional LSTM network and $\Delta \mathrm {T} = 5$, we plot boxplots (inside the box are values from the lower to the upper quartile (25th to 75th percentile), the orange line represents the median and the vertical lines represent the values that lie in 1.5 times the interquartile range (height of the box)) of the optimal \textbf {(a)}, predicted \textbf {(b)} and absolute differences \textbf {(c)} of $\theta ^n_T$ for the five qubits of each trajectory in the test set. The prediction for $\theta _T^N$ is very good as the optimal solution is to set it to $\frac {\pi }{2}$ in all cases to maximise the work output of the final step. The prediction for $\theta _T^0$ is reasonably good too, as the optimal solution is again $\frac {\pi }{2}$ for a majority of trajectories. The network is unable to predict the three central qubits, with median absolute differences of approximately $0.5$. \textbf {(d)}: we plot the absolute difference between optimal and predicted $\phi _T^n$. We refrain from plotting the optimal and predicted $\phi _T^n$ themselves as these are distributed uniformly.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces We plot the performance of the bidirectional LSTM for $\Delta \mathrm  {T} = 1$. Unlike the optimum for the longer switching time, $\theta _T^n = \frac  {\pi }{2}$ for a majority of the central three qubits as well. Again, predicted and optimal $\phi ^n_T$ are distributed uniformly and therefore we do not plot them here.\relax }}{21}{figure.caption.15}\protected@file@percent }
\newlabel{dt1box}{{4.6}{21}{We plot the performance of the bidirectional LSTM for $\Delta \mathrm {T} = 1$. Unlike the optimum for the longer switching time, $\theta _T^n = \frac {\pi }{2}$ for a majority of the central three qubits as well. Again, predicted and optimal $\phi ^n_T$ are distributed uniformly and therefore we do not plot them here.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces We plot the difference $\Delta W = \overline  {W}_{noise} - W_{pred}$ of each element of the test set for the bidirectional LSTM. $W_{pred}$ is the work output following the model prediction. \textbf  {(a), (b)}: we create 100 noisy drive sequences and calculate the average $\overline  {W}_{noise}$ of their work output following the predicted transducer protocol. \textbf  {(c), (d)}: we create 100 noisy transducer sequences and calculate the average of their work output with a given drive sequence from the test set. In both plots, the black dots indicate the average fidelities and $\Delta W$ for a given $\tau $.\relax }}{22}{figure.caption.16}\protected@file@percent }
\newlabel{noisedt5}{{4.7}{22}{We plot the difference $\Delta W = \overline {W}_{noise} - W_{pred}$ of each element of the test set for the bidirectional LSTM. $W_{pred}$ is the work output following the model prediction. \textbf {(a), (b)}: we create 100 noisy drive sequences and calculate the average $\overline {W}_{noise}$ of their work output following the predicted transducer protocol. \textbf {(c), (d)}: we create 100 noisy transducer sequences and calculate the average of their work output with a given drive sequence from the test set. In both plots, the black dots indicate the average fidelities and $\Delta W$ for a given $\tau $.\relax }{figure.caption.16}{}}
\BKM@entry{id=20,dest={73656374696F6E2E342E35},srcline={120}}{47656E6572616C69736174696F6E20746F206F74686572204E}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Generalisation to other $N$}{23}{section.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Generalisability of MSE and local optimisation models. For each $N$, we generate 1000 random drives and predict their transducer policies. We plot the mean of their work output $\overline  {W}$ for varying $N$. Where available, we plot the optimal average work output. For $\Delta \mathrm  {T} = 1$, we additionally plot $\overline  {W}$ for the bidirectional LSTM trained using the extracted work as the loss function. The average output of the local optimisation protocol is plotted as a benchmark.\relax }}{23}{figure.caption.17}\protected@file@percent }
\newlabel{genplot}{{4.8}{23}{Generalisability of MSE and local optimisation models. For each $N$, we generate 1000 random drives and predict their transducer policies. We plot the mean of their work output $\overline {W}$ for varying $N$. Where available, we plot the optimal average work output. For $\Delta \mathrm {T} = 1$, we additionally plot $\overline {W}$ for the bidirectional LSTM trained using the extracted work as the loss function. The average output of the local optimisation protocol is plotted as a benchmark.\relax }{figure.caption.17}{}}
\BKM@entry{id=21,dest={636861707465722E35},srcline={123}}{436F6E636C7573696F6E20616E64206F75746C6F6F6B}
\citation{Banchi_2018}
\citation{PhysRevX.10.011006}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion and outlook}{25}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{outlook}{{5}{25}{Conclusion and outlook}{chapter.5}{}}
\citation{Sutton1998}
\bibstyle{unsrt}
\bibdata{biblio}
\BKM@entry{id=22,dest={636861707465722E36},srcline={1}}{4269626C696F677261706879}
\bibcite{thomson_2011}{1}
\bibcite{1905AnP...322..132E}{2}
\bibcite{Egloff_2015}{3}
\bibcite{PhysRevE.93.022131}{4}
\bibcite{beyer2020}{5}
\bibcite{WEI20171}{6}
\bibcite{expharv}{7}
\bibcite{sothmann}{8}
\bibcite{Liu2019}{9}
\bibcite{DBLP:journals/corr/VaswaniSPUJGKP17}{10}
\bibcite{Carleo_2019}{11}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Bibliography}{27}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{wise2021using}{12}
\bibcite{8614252}{13}
\bibcite{Lorenzo_2017}{14}
\bibcite{Mitchell97}{15}
\bibcite{lu2020dying}{16}
\bibcite{TN_libero_mab2)53517}{17}
\bibcite{Maas2013RectifierNI}{18}
\bibcite{krizhevsky}{19}
\bibcite{LeNail2019}{20}
\bibcite{10.1007/978-3-642-46466-9_18}{21}
\bibcite{doi:10.1162/neco.1997.9.8.1735}{22}
\bibcite{NEURIPS2019_9015}{23}
\bibcite{bidirrnn}{24}
\bibcite{rumelhart1986learning}{25}
\bibcite{nielsenneural}{26}
\bibcite{kingma2017adam}{27}
\bibcite{2020SciPy-NMeth}{28}
\bibcite{Deffner_2017}{29}
\bibcite{PhysRevA.67.052109}{30}
\bibcite{Mezzadri}{31}
\bibcite{LeCun2012}{32}
\bibcite{10.5555/1972505}{33}
\bibcite{Banchi_2018}{34}
\bibcite{PhysRevX.10.011006}{35}
\bibcite{Sutton1998}{36}
\bibcite{hinton2012improving}{37}
\bibcite{10.5555/2999325.2999464}{38}
\bibcite{wandb}{39}
\BKM@entry{id=23,dest={617070656E6469782E41},srcline={130}}{44657269766174696F6E73}
\BKM@entry{id=24,dest={73656374696F6E2E412E31},srcline={131}}{53696E676C652065787472616374696F6E207374657020776F726B206F7574707574}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Derivations}{31}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Single extraction step work output}{31}{section.A.1}\protected@file@percent }
\BKM@entry{id=25,dest={73656374696F6E2E412E32},srcline={134}}{4F7074696D616C20706F6C69637920666F72204E3D32}
\newlabel{dwformula}{{A.1}{32}{Single extraction step work output}{equation.A.1.1}{}}
\newlabel{deriv_jump}{{A.1}{32}{Single extraction step work output}{equation.A.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Optimal policy for $N=2$}{32}{section.A.2}\protected@file@percent }
\newlabel{n2_opt_pol}{{A.2}{32}{Optimal policy for $N=2$}{section.A.2}{}}
\newlabel{commutator}{{A.2}{32}{Optimal policy for $N=2$}{equation.A.2.2}{}}
\newlabel{phiopt}{{A.3}{32}{Optimal policy for $N=2$}{equation.A.2.3}{}}
\newlabel{singleopt}{{A.5}{32}{Optimal policy for $N=2$}{equation.A.2.5}{}}
\BKM@entry{id=26,dest={617070656E6469782E42},srcline={136}}{547261696E696E672070726F746F636F6C73}
\citation{NEURIPS2019_9015}
\citation{hinton2012improving}
\BKM@entry{id=27,dest={73656374696F6E2E422E31},srcline={17}}{4879706572706172616D65746572732053656374696F6E20342E33}
\citation{kingma2017adam}
\BKM@entry{id=28,dest={73656374696F6E2E422E32},srcline={36}}{4879706572706172616D65746572732053656374696F6E20342E34}
\citation{10.5555/2999325.2999464}
\citation{wandb}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Training protocols}{33}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{training}{{B}{33}{Training protocols}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Hyperparameters Section \ref  {n_2_ml}}{33}{section.B.1}\protected@file@percent }
\newlabel{eds}{{B.1}{33}{Hyperparameters Section \ref {n_2_ml}}{equation.B.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Hyperparameters used in training for the models in section \ref  {n_2_ml}.\relax }}{33}{table.caption.18}\protected@file@percent }
\newlabel{hyperparams_n_2}{{B.1}{33}{Hyperparameters used in training for the models in section \ref {n_2_ml}.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Hyperparameters Section \ref  {n5}}{33}{section.B.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Hyperparameters used for the models in Sections \ref  {n_5_ml} to \ref  {ml_noise}.\relax }}{34}{table.caption.19}\protected@file@percent }
\newlabel{hyperparams_n_5}{{B.2}{34}{Hyperparameters used for the models in Sections \ref {n_5_ml} to \ref {ml_noise}.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Hyperparameters used for the model in Section \ref  {work_cost}.\relax }}{34}{table.caption.20}\protected@file@percent }
\newlabel{hyperparams_work_cost}{{B.3}{34}{Hyperparameters used for the model in Section \ref {work_cost}.\relax }{table.caption.20}{}}
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{18.10382pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{25.90468pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{32.1087pt}
