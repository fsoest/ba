\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{nil}{}
\BKM@entry{id=1,dest={636861707465722E31},srcline={86}}{496E74726F64756374696F6E}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\citation{Carleo_2019}
\citation{wise2021using}
\citation{Liu2019}
\citation{beyer2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=2,dest={636861707465722E32},srcline={89}}{4261636B67726F756E64}
\BKM@entry{id=3,dest={73656374696F6E2E322E31},srcline={90}}{436F6C6C6973696F6E206D6F64656C2064796E616D696373}
\citation{Lorenzo_2017}
\citation{beyer2020}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{background}{{2}{3}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Collision model dynamics}{3}{section.2.1}\protected@file@percent }
\newlabel{col_model}{{2.1}{3}{Collision model dynamics}{section.2.1}{}}
\newlabel{coll_eq}{{2.1}{3}{Collision model dynamics}{equation.2.1.1}{}}
\newlabel{relham}{{2.2}{4}{Collision model dynamics}{equation.2.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Piecewise constant implementation of Drive and Transducer qubits: the vertical axis represents an arbitrary parameter of the ancilla states. The qubit states are switched instantaneously and then kept constant for $\Delta \mathrm  {T}$ while $\rho _S$ evolves unitarily. The piece-wise constant Drive and Transducer settings lead to a piece-wise constant system Hamiltonian.\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{pwc}{{2.1}{5}{Piecewise constant implementation of Drive and Transducer qubits: the vertical axis represents an arbitrary parameter of the ancilla states. The qubit states are switched instantaneously and then kept constant for $\Delta \mathrm {T}$ while $\rho _S$ evolves unitarily. The piece-wise constant Drive and Transducer settings lead to a piece-wise constant system Hamiltonian.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Collision model used in this work: Drive and Transducer are series of qubits that interact once with the system and evolve the reduced density operator $\rho _S$. The qubit configuration can be changed in intervals of $\Delta \mathrm  {T}$.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{collmodel}{{2.2}{5}{Collision model used in this work: Drive and Transducer are series of qubits that interact once with the system and evolve the reduced density operator $\rho _S$. The qubit configuration can be changed in intervals of $\Delta \mathrm {T}$.\relax }{figure.caption.3}{}}
\BKM@entry{id=4,dest={73656374696F6E2E322E32},srcline={95}}{53757065727669736564204D616368696E65204C6561726E696E67}
\citation{Mitchell97}
\BKM@entry{id=5,dest={73756273656374696F6E2E322E322E31},srcline={9}}{46756C6C792D636F6E6E65637465642066656564666F727761726420414E4E73}
\citation{lu2020dying}
\citation{TN_libero_mab2)53517}
\citation{Maas2013RectifierNI}
\citation{krizhevsky}
\citation{LeNail2019}
\citation{LeNail2019}
\BKM@entry{id=6,dest={73756273656374696F6E2E322E322E32},srcline={29}}{4C6F6E672053686F72742D5465726D204D656D6F7279}
\citation{rumelhart1986learning}
\citation{10.1007/978-3-642-46466-9_18}
\citation{doi:10.1162/neco.1997.9.8.1735}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Supervised Machine Learning}{6}{section.2.2}\protected@file@percent }
\newlabel{sml}{{2.2}{6}{Supervised Machine Learning}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Fully-connected feedforward ANNs}{6}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Long Short-Term Memory}{6}{subsection.2.2.2}\protected@file@percent }
\citation{NEURIPS2019_9015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example fully-connected feedforward ANN with four layers, including input, output and two hidden layers \cite  {LeNail2019}.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{nn}{{2.3}{7}{Example fully-connected feedforward ANN with four layers, including input, output and two hidden layers \cite {LeNail2019}.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example two-layer RNN with LSTM architecture. Each row of LSTM blocks has the same parameters and information is fed into the network sequentially.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{rnn}{{2.4}{7}{Example two-layer RNN with LSTM architecture. Each row of LSTM blocks has the same parameters and information is fed into the network sequentially.\relax }{figure.caption.5}{}}
\citation{bidirrnn}
\BKM@entry{id=7,dest={73756273656374696F6E2E322E322E33},srcline={69}}{547261696E696E67205C303436204261636B70726F7061676174696F6E}
\citation{rumelhart1986learning}
\citation{nielsenneural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Training \& Backpropagation}{8}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualisation of a single LSTM cell. The input $x_t$ enters the cell in the bottom left and is concatenated with the output $h_{t\text  {-}1}$ of the previous cell. The combined data then enters multiple single-layer neural networks represented by orange rectangles with their respective activation function. The forget gate $f_t$ removes data from the previous cell state $c_{t\text  {-}1}$ while $i_t$ and $g_t$ control how new data is added to the memory. $\tanh $ is applied elementwise over the cell state (yellow ellipse) to determine the cell output $h_t$ together with the output gate $o_t$.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{lstm}{{2.5}{9}{Visualisation of a single LSTM cell. The input $x_t$ enters the cell in the bottom left and is concatenated with the output $h_{t\text {-}1}$ of the previous cell. The combined data then enters multiple single-layer neural networks represented by orange rectangles with their respective activation function. The forget gate $f_t$ removes data from the previous cell state $c_{t\text {-}1}$ while $i_t$ and $g_t$ control how new data is added to the memory. $\tanh $ is applied elementwise over the cell state (yellow ellipse) to determine the cell output $h_t$ together with the output gate $o_t$.\relax }{figure.caption.6}{}}
\BKM@entry{id=8,dest={636861707465722E33},srcline={99}}{4578706572696D656E74616C20526573756C7473}
\citation{2020SciPy-NMeth}
\citation{LeCun2012}
\BKM@entry{id=9,dest={73656374696F6E2E332E31},srcline={101}}{496E666C75656E6365206F662054206F6E20576F726B204F7574707574}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Experimental Results}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Influence of $\Delta \mathrm  {T}$ on Work Output}{11}{section.3.1}\protected@file@percent }
\newlabel{dep_dt}{{3.1}{11}{Influence of $\Delta \mathrm {T}$ on Work Output}{section.3.1}{}}
\citation{Deffner_2017}
\citation{PhysRevA.67.052109}
\newlabel{single_work}{{3.2}{12}{Influence of $\Delta \mathrm {T}$ on Work Output}{equation.3.1.2}{}}
\BKM@entry{id=10,dest={73656374696F6E2E332E32},srcline={103}}{4E3D323A204C6561726E696E672053696E676C65204A756D70204F7074696D616C20436F6E74726F6C2053657175656E636573}
\citation{Mezzadri}
\BKM@entry{id=11,dest={73656374696F6E2E332E33},srcline={105}}{4E3D35}
\BKM@entry{id=12,dest={73756273656374696F6E2E332E332E31},srcline={106}}{54203D2035}
\newlabel{dt_0}{{3.1a}{13}{$\rho _0 = \ket {0}\bra {0}$\relax }{figure.caption.7}{}}
\newlabel{sub@dt_0}{{a}{13}{$\rho _0 = \ket {0}\bra {0}$\relax }{figure.caption.7}{}}
\newlabel{dt_eigen}{{3.1b}{13}{$\rho _0 = \ket {+}\bra {+}$\relax }{figure.caption.7}{}}
\newlabel{sub@dt_eigen}{{b}{13}{$\rho _0 = \ket {+}\bra {+}$\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces (a) We plot the average work $\overline  {W}$ over $n = 500$ runs of random excitations divided by amount of qubit changes $N - 1$, with $\rho _0 = \ket {0}\bra {0}$, for multiple $N$. The error bars correspond to the standard deviation $\sigma _{W} = \sqrt  {\frac  {1}{n-1} \Sigma _i^n (\overline  {W} - W_i)^2}$. (b) We plot $\overline  {W}/(N-1)$ for multiple $N$ where the system state is initialised in an eigenstate of the Drive Hamiltonian $H_{DS}$.\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{dt_dep}{{3.1}{13}{(a) We plot the average work $\overline {W}$ over $n = 500$ runs of random excitations divided by amount of qubit changes $N - 1$, with $\rho _0 = \ket {0}\bra {0}$, for multiple $N$. The error bars correspond to the standard deviation $\sigma _{W} = \sqrt {\frac {1}{n-1} \Sigma _i^n (\overline {W} - W_i)^2}$. (b) We plot $\overline {W}/(N-1)$ for multiple $N$ where the system state is initialised in an eigenstate of the Drive Hamiltonian $H_{DS}$.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}$N=2$: Learning Single Jump Optimal Control Sequences}{13}{section.3.2}\protected@file@percent }
\newlabel{n_2_ml}{{3.2}{13}{$N=2$: Learning Single Jump Optimal Control Sequences}{section.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Efficiencies $\eta $ on the test data for models with a single hidden layer with 10 neurons trained on Drive protocols with $N = 2$ and differing initial states $\rho _0$.\relax }}{13}{table.caption.8}\protected@file@percent }
\newlabel{n2efftable}{{3.1}{13}{Efficiencies $\eta $ on the test data for models with a single hidden layer with 10 neurons trained on Drive protocols with $N = 2$ and differing initial states $\rho _0$.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}$N=5$}{14}{section.3.3}\protected@file@percent }
\newlabel{n5}{{3.3}{14}{$N=5$}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}$\Delta \mathrm  {T} = 5$}{14}{subsection.3.3.1}\protected@file@percent }
\newlabel{n_5_ml}{{3.3.1}{14}{$\Delta \mathrm {T} = 5$}{subsection.3.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Efficiencies $\eta $ on the test data for model architectures with given number of trainable parameters for $N=5, \Delta \mathrm  {T} = 5$.\relax }}{14}{table.caption.10}\protected@file@percent }
\newlabel{n5efftable}{{3.2}{14}{Efficiencies $\eta $ on the test data for model architectures with given number of trainable parameters for $N=5, \Delta \mathrm {T} = 5$.\relax }{table.caption.10}{}}
\BKM@entry{id=13,dest={73756273656374696F6E2E332E332E32},srcline={109}}{54203D2031}
\BKM@entry{id=14,dest={73756273656374696F6E2E332E332E33},srcline={112}}{4E6F69736520726573697374616E6365}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Architecture of the LSTM networks. The green blocks represent the unidirectional case. For the bidirectional case, a second LSTM block (orange) is included with separate trainable parameters. The blocks labeled `FC' represent fully-connected layers.\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{lstm_network}{{3.2}{15}{Architecture of the LSTM networks. The green blocks represent the unidirectional case. For the bidirectional case, a second LSTM block (orange) is included with separate trainable parameters. The blocks labeled `FC' represent fully-connected layers.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}$\Delta \mathrm  {T} = 1$}{15}{subsection.3.3.2}\protected@file@percent }
\newlabel{n_5_dt1}{{3.3.2}{15}{$\Delta \mathrm {T} = 1$}{subsection.3.3.2}{}}
\newlabel{bloch_10553}{{3.3a}{16}{$W_{opt} = 3.03, W_{pred} = 1.11$\relax }{figure.caption.11}{}}
\newlabel{sub@bloch_10553}{{a}{16}{$W_{opt} = 3.03, W_{pred} = 1.11$\relax }{figure.caption.11}{}}
\newlabel{bloch_worst}{{3.3b}{16}{$W_{opt} = 3.06, W_{pred} = -2.65$\relax }{figure.caption.11}{}}
\newlabel{sub@bloch_worst}{{b}{16}{$W_{opt} = 3.06, W_{pred} = -2.65$\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \textbf  {(a)} We plot the evolution of a single sample from the test set for $N=5$ and $\Delta \mathrm  {T} = 5$. For this sample we have $W_{opt} = 3.03, W_{pred} = 1.11$. Each Bloch sphere shows $\rho _S^i$ (blue dot), $\rho _S^{i+1}$ (black square), the partial system Hamiltonian acting only between system and Drive $H_{DS}^i$ (red vector) as well as $H_{ST}^i$ (yellow vector) and $H_{ST}^{i+1}$ (green vector). \textbf  {Top row:} we plot the system dynamics for the Transducer series generated by the optimiser for $i \in [1, N - 1]$. In the optimal case, $H_{ST}^i$ is chosen such that $\rho _S$ remains near the x-y-plane for all times and $\Tr {\rho _S^{i+1} H_{DS}^i}$ is large. \textbf  {Bottom row:} we plot the dynamics for the same Drive protocol with Transducer qubits predicted by the bidirectional LSTM. The overall difference of $\rho _S$ to the x-y-plane is larger. As shown in Figure \ref  {bilstmbox}, $\theta _T$ is often set to $\frac  {\pi }{2}$ which maximises the strength of $H_{ST}$ as can be seen in all Bloch spheres in the bottom row. In this case, the $H_{DS}$ are chosen by the network to be antiparallel, irrespective of the current system state. \textbf  {(b)} We show the same plot as in (a) for the worst performing sample from the test set to illustrate a shortcoming of the model. As can be seen from the yellow and green vectors, the predictions for $j \in [2, N]$ are very close to the optimal solutions. However, the first Transducer prediction is wrong, leading to a deviation from the optimal system dynamics.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{n_5_blochs}{{3.3}{16}{\textbf {(a)} We plot the evolution of a single sample from the test set for $N=5$ and $\Delta \mathrm {T} = 5$. For this sample we have $W_{opt} = 3.03, W_{pred} = 1.11$. Each Bloch sphere shows $\rho _S^i$ (blue dot), $\rho _S^{i+1}$ (black square), the partial system Hamiltonian acting only between system and Drive $H_{DS}^i$ (red vector) as well as $H_{ST}^i$ (yellow vector) and $H_{ST}^{i+1}$ (green vector). \textbf {Top row:} we plot the system dynamics for the Transducer series generated by the optimiser for $i \in [1, N - 1]$. In the optimal case, $H_{ST}^i$ is chosen such that $\rho _S$ remains near the x-y-plane for all times and $\Tr {\rho _S^{i+1} H_{DS}^i}$ is large. \textbf {Bottom row:} we plot the dynamics for the same Drive protocol with Transducer qubits predicted by the bidirectional LSTM. The overall difference of $\rho _S$ to the x-y-plane is larger. As shown in Figure \ref {bilstmbox}, $\theta _T$ is often set to $\frac {\pi }{2}$ which maximises the strength of $H_{ST}$ as can be seen in all Bloch spheres in the bottom row. In this case, the $H_{DS}$ are chosen by the network to be antiparallel, irrespective of the current system state. \textbf {(b)} We show the same plot as in (a) for the worst performing sample from the test set to illustrate a shortcoming of the model. As can be seen from the yellow and green vectors, the predictions for $j \in [2, N]$ are very close to the optimal solutions. However, the first Transducer prediction is wrong, leading to a deviation from the optimal system dynamics.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \textbf  {Top row:} for the bidirectional LSTM network and $\Delta \mathrm  {T} = 5$, we plot boxplots of the optimal, predicted and absolute differences of $\theta _T$ for the five qubits of each trajectory in the test set. The prediction for $\theta _T^N$ is very good as the optimal solution is to set it to $\frac  {\pi }{2}$ in all cases to maximise the work output of the final step. The prediction for $\theta _T^0$ is reasonably good as well, as the optimal solution is again $\frac  {\pi }{2}$ in a majority of trajectories. The network is unable to predict the three central qubits, with median absolute differences of approximately $0.5$. \textbf  {Bottom row:} we plot the same quantities as above for the Transducer azimuth $\phi _T$.\relax }}{17}{figure.caption.12}\protected@file@percent }
\newlabel{bilstmbox}{{3.4}{17}{\textbf {Top row:} for the bidirectional LSTM network and $\Delta \mathrm {T} = 5$, we plot boxplots of the optimal, predicted and absolute differences of $\theta _T$ for the five qubits of each trajectory in the test set. The prediction for $\theta _T^N$ is very good as the optimal solution is to set it to $\frac {\pi }{2}$ in all cases to maximise the work output of the final step. The prediction for $\theta _T^0$ is reasonably good as well, as the optimal solution is again $\frac {\pi }{2}$ in a majority of trajectories. The network is unable to predict the three central qubits, with median absolute differences of approximately $0.5$. \textbf {Bottom row:} we plot the same quantities as above for the Transducer azimuth $\phi _T$.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \textbf  {Top row:} we plot the performance of the bidirectional LSTM on $\theta _T$ for $\Delta \mathrm  {T} = 1$. Unlike the optimum for the longer switching time, $\theta _T = \frac  {\pi }{2}$ for a majority of the central three qubits as well. For $\Delta \mathrm  {T} = 5$, the optimal $\theta _T$ is chosen such that $\Tr {\rho _S \ \sigma _z} = 0$. These states are often unreachable for $\Delta \mathrm  {T} = 1$ and the Transducer Hamiltonian strength is maximised instead. \textbf  {Bottom row:} we plot the performance for the Transducer azimuth $\phi _T$.\relax }}{17}{figure.caption.13}\protected@file@percent }
\newlabel{dt1box}{{3.5}{17}{\textbf {Top row:} we plot the performance of the bidirectional LSTM on $\theta _T$ for $\Delta \mathrm {T} = 1$. Unlike the optimum for the longer switching time, $\theta _T = \frac {\pi }{2}$ for a majority of the central three qubits as well. For $\Delta \mathrm {T} = 5$, the optimal $\theta _T$ is chosen such that $\Tr {\rho _S \ \sigma _z} = 0$. These states are often unreachable for $\Delta \mathrm {T} = 1$ and the Transducer Hamiltonian strength is maximised instead. \textbf {Bottom row:} we plot the performance for the Transducer azimuth $\phi _T$.\relax }{figure.caption.13}{}}
\newlabel{}{{3.6a}{18}{$\Delta \mathrm {T} = 1: W_{opt} = 1.40, W_{pred} = 1.32$\relax }{figure.caption.15}{}}
\newlabel{sub@}{{a}{18}{$\Delta \mathrm {T} = 1: W_{opt} = 1.40, W_{pred} = 1.32$\relax }{figure.caption.15}{}}
\newlabel{}{{3.6b}{18}{$\Delta \mathrm {T} = 5: W_{opt} = 2.42, W_{pred} = 0.57$\relax }{figure.caption.15}{}}
\newlabel{sub@}{{b}{18}{$\Delta \mathrm {T} = 5: W_{opt} = 2.42, W_{pred} = 0.57$\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces We plot the optimal (top row) and predicted (bottom row) system state and Transducer settings for the bidirectional LSTM, using the same representations as in Figure \ref  {n_5_blochs}, for $\Delta \mathrm  {T} = 1$ (a) and $\Delta \mathrm  {T} = 5$ (b). The Drive sequence is the same in all rows and is a sample from the test set. For $\Delta \mathrm  {T} = 1$, the optimal system state, where $\Tr {\rho _S^i \sigma _z} = 0$, cannot be reached.\relax }}{18}{figure.caption.15}\protected@file@percent }
\newlabel{}{{3.6}{18}{We plot the optimal (top row) and predicted (bottom row) system state and Transducer settings for the bidirectional LSTM, using the same representations as in Figure \ref {n_5_blochs}, for $\Delta \mathrm {T} = 1$ (a) and $\Delta \mathrm {T} = 5$ (b). The Drive sequence is the same in all rows and is a sample from the test set. For $\Delta \mathrm {T} = 1$, the optimal system state, where $\Tr {\rho _S^i \sigma _z} = 0$, cannot be reached.\relax }{figure.caption.15}{}}
\citation{10.5555/1972505}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Efficiencies $\eta $ and MSE loss on the test set for differing model architectures for $N=5, \Delta \mathrm  {T} = 1$.\relax }}{19}{table.caption.14}\protected@file@percent }
\newlabel{effdt1}{{3.3}{19}{Efficiencies $\eta $ and MSE loss on the test set for differing model architectures for $N=5, \Delta \mathrm {T} = 1$.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Noise resistance}{19}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces We plot the difference $\Delta W = \overline  {W}_{noise} - W_{pred}$ of each element of the test set for the bidirectional LSTM. $W_{pred}$ is the work output following the model prediction. \textbf  {(a), (b)} We create 100 noisy Drive sequences and calculate the average $\overline  {W}_{noise}$ of their work output following the predicted Transducer protocol. \textbf  {(c), (d)} We create 100 noisy Transducer sequences and calculate the average of their work output with a given Drive sequence from the test set. In both plots, the black dots indicate the averaged fidelities and $\Delta W$ for a given $\tau $.\relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{noisedt5}{{3.7}{20}{We plot the difference $\Delta W = \overline {W}_{noise} - W_{pred}$ of each element of the test set for the bidirectional LSTM. $W_{pred}$ is the work output following the model prediction. \textbf {(a), (b)} We create 100 noisy Drive sequences and calculate the average $\overline {W}_{noise}$ of their work output following the predicted Transducer protocol. \textbf {(c), (d)} We create 100 noisy Transducer sequences and calculate the average of their work output with a given Drive sequence from the test set. In both plots, the black dots indicate the averaged fidelities and $\Delta W$ for a given $\tau $.\relax }{figure.caption.16}{}}
\BKM@entry{id=15,dest={73656374696F6E2E332E34},srcline={115}}{45787472616374656420776F726B20617320636F73742066756E6374696F6E}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Extracted work as cost function}{21}{section.3.4}\protected@file@percent }
\newlabel{work_cost}{{3.4}{21}{Extracted work as cost function}{section.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Model performance for differing network architectures and two switching times $\Delta \mathrm  {T}$.\relax }}{21}{table.caption.17}\protected@file@percent }
\newlabel{workcosttable}{{3.4}{21}{Model performance for differing network architectures and two switching times $\Delta \mathrm {T}$.\relax }{table.caption.17}{}}
\BKM@entry{id=16,dest={636861707465722E34},srcline={118}}{53756D6D61727920616E64204F75746C6F6F6B}
\citation{Banchi_2018}
\citation{PhysRevX.10.011006}
\bibstyle{plain}
\bibdata{biblio}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Summary and Outlook}{23}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{outlook}{{4}{23}{Summary and Outlook}{chapter.4}{}}
\BKM@entry{id=17,dest={636861707465722E35},srcline={1}}{4269626C696F677261706879}
\bibcite{Banchi_2018}{1}
\bibcite{beyer2020}{2}
\bibcite{wandb}{3}
\bibcite{Carleo_2019}{4}
\bibcite{Deffner_2017}{5}
\bibcite{PhysRevX.10.011006}{6}
\bibcite{10.1007/978-3-642-46466-9_18}{7}
\bibcite{PhysRevA.67.052109}{8}
\bibcite{hinton2012improving}{9}
\bibcite{doi:10.1162/neco.1997.9.8.1735}{10}
\bibcite{krizhevsky}{11}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Bibliography}{25}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{LeCun2012}{12}
\bibcite{LeNail2019}{13}
\bibcite{Liu2019}{14}
\bibcite{Lorenzo_2017}{15}
\bibcite{lu2020dying}{16}
\bibcite{Maas2013RectifierNI}{17}
\bibcite{Mezzadri}{18}
\bibcite{Mitchell97}{19}
\bibcite{nielsenneural}{20}
\bibcite{10.5555/1972505}{21}
\bibcite{NEURIPS2019_9015}{22}
\bibcite{rumelhart1986learning}{23}
\bibcite{bidirrnn}{24}
\bibcite{DBLP:journals/corr/VaswaniSPUJGKP17}{25}
\bibcite{2020SciPy-NMeth}{26}
\bibcite{wise2021using}{27}
\bibcite{TN_libero_mab2)53517}{28}
\BKM@entry{id=18,dest={617070656E6469782E41},srcline={128}}{44657269766174696F6E73}
\BKM@entry{id=19,dest={73656374696F6E2E412E31},srcline={129}}{53696E676C65206A756D7020776F726B206F7574707574}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Derivations}{29}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Single jump work output}{29}{section.A.1}\protected@file@percent }
\newlabel{deriv_jump}{{A.1}{29}{Single jump work output}{section.A.1}{}}
\BKM@entry{id=20,dest={73656374696F6E2E412E32},srcline={132}}{4F7074696D616C20706F6C69637920666F72204E3D32}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Optimal policy for $N=2$}{30}{section.A.2}\protected@file@percent }
\newlabel{n2_opt_pol}{{A.2}{30}{Optimal policy for $N=2$}{section.A.2}{}}
\newlabel{commutator}{{A.1}{30}{Optimal policy for $N=2$}{equation.A.2.1}{}}
\BKM@entry{id=21,dest={617070656E6469782E42},srcline={134}}{547261696E696E672070726F746F636F6C73}
\citation{NEURIPS2019_9015}
\citation{hinton2012improving}
\BKM@entry{id=22,dest={73656374696F6E2E422E31},srcline={10}}{4879706572706172616D65746572732073656374696F6E20332E32}
\BKM@entry{id=23,dest={73656374696F6E2E422E32},srcline={29}}{4879706572706172616D657465727320332E332E31}
\citation{wandb}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Training protocols}{31}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Hyperparameters section \ref  {n_2_ml}}{31}{section.B.1}\protected@file@percent }
\newlabel{eds}{{B.1}{31}{Hyperparameters section \ref {n_2_ml}}{equation.B.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Hyperparameters used in training for the models in section \ref  {n_2_ml}.\relax }}{31}{table.caption.18}\protected@file@percent }
\newlabel{hyperparams_n_2}{{B.1}{31}{Hyperparameters used in training for the models in section \ref {n_2_ml}.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Hyperparameters \ref  {n_5_ml}}{31}{section.B.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Hyperparameters used for the models in Sections \ref  {n5} and \ref  {work_cost}.\relax }}{32}{table.caption.19}\protected@file@percent }
\newlabel{hyperparams_n_5}{{B.2}{32}{Hyperparameters used for the models in Sections \ref {n5} and \ref {work_cost}.\relax }{table.caption.19}{}}
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{18.10382pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{25.90468pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{32.1087pt}
