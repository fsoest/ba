% !TeX spellcheck = en_GB
In all models we split the data into training, validation and test data sets with $p_{test} = 18 \%$ and $p_{valid} = 8.2 \%$.
The models are implemented using the PyTorch library \cite{NEURIPS2019_9015}. 
Training is performed over $n$ epochs, in each of which the complete training data set is used once in batches of \textit{batch size}.
In each batch, the gradient of the loss function with regard to the model parameters is calculated and the parameters updated.

A common problem in machine learning is \textit{overfitting}, where the model learns noise in the data instead of the underlying functional correlation.
It is indicated by a good performance on the training data but poor performance on unseen data.
We employ two techniques to prevent overfitting.
Firstly we use early stopping to stop training when the cost function on the validation set (unseen data) does not decrease for \textit{patience} epochs.
Additionally, we apply dropout after all layers except for input and output:
during training, each neuron is deactivated with probability \textit{dropout}.
This has been proven to prevent overfitting \cite{hinton2012improving}.

All performance data is calculated on the test set, which the model has seen neither during training nor has been used for early stopping.

\section{Hyperparameters Section \ref{n_2_ml}}
For $N=2$, we use the Adam \cite{kingma2017adam} optimiser with $\beta_1 = 0.9$, $\beta_2 = 0.98$ and $\epsilon = 10^{-9}$. The learning rate (LR) is determined by an exponential decay schedule
\begin{equation} \label{eds}
\mathrm{LR} \ (i) = \mathrm{initial \ LR} * \mathrm{decay \ rate}^i,
\end{equation}
where $i$ denotes the optimiser step.


\begin{table}[h]
	\centering
	\begin{tabular}{c | c | c }
		Patience & Initial LR & Decay Rate \\
		\hline
		100 & $10^{-2}$ & 0.995 \\
	\end{tabular}
	\caption{Hyperparameters used in training for the models in section \ref{n_2_ml}.}
	\label{hyperparams_n_2}
\end{table}

\section{Hyperparameters Section \ref{n5}}
The learning rate (LR) is determined by the `Reduce LR on Plateau' schedule:
the learning rate is multiplied by the hyperparameter \textit{factor} if the cost function has not decreased for the past \textit{patience} / \textit{pat drop} epochs.

The hyperparameters are found using Bayesian optimisation \cite{10.5555/2999325.2999464}, which uses multiple training runs with different hyperparameters to create a statistical model of the model performance with respect to the hyperparameters, implemented by \cite{wandb}.
This model is used to predict new hyperparameters for training which are likely to perform well.
The values for the models in Sections \ref{n_5_ml} to \ref{ml_noise} can be found in Table \ref{hyperparams_n_5}.
The FCANN in Section \ref{n_5_ml} has three hidden layers with 2000 neurons each.
The hyperparameters for the bidirectional LSTM from Section \ref{work_cost} are presented in Table \ref{hyperparams_work_cost}.

\begin{table}[h]
	\centering
	\begin{tabular}{l | l}
		Hyperparameter & Value \\
		\hline
		Optimiser & SGD \\
		Batch size & 44 \\
		Dropout & 0.3179732914255167 \\
		Input layer 1 size & 793 \\
		Input layer 2 size & 488 \\
		Output layer size & 228 \\
		LSTM hidden size & 324 \\
		Initial learning rate & 0.02847560288866327 \\
		LSTM layers & 3 \\
		Pat drop & 3.698498258242224 \\
		Patience & 55 \\
		Factor & 0.24509238889070978
	\end{tabular}
	\caption{Hyperparameters used for the models in Sections \ref{n_5_ml} to \ref{ml_noise}.}
	\label{hyperparams_n_5}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{l | l}
		Hyperparameter & Value \\
		\hline
		Optimiser & SGD \\
		Batch size & 23 \\
		Dropout & 0.1343610845377841 \\
		Input layer 1 size & 58 \\
		Input layer 2 size & 1115 \\
		Output layer size & 738 \\
		LSTM hidden size & 528 \\
		Initial learning rate & 0.0484126686015994 \\
		LSTM layers & 1 \\
		Pat drop & 1.5006100847416177 \\
		Patience & 78 \\
		Factor & 0.5967308787201352
	\end{tabular}
	\caption{Hyperparameters used for the model in Section \ref{work_cost}.}
	\label{hyperparams_work_cost}
\end{table}