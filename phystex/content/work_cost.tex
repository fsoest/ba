% !TeX root = ../BA_main_englisch.tex
% !TeX spellcheck = en_GB
In Section \ref{n_5_ml} we encountered a conceptional problem regarding using the MSE as a loss function, where a low MSE score does not necessarily imply a large work output.
In the following we aim to investigate whether directly using extracted work $W$ as the network cost function can improve performance.

For $\Delta \mathrm{T} = 1$, we train a bidirectional LSTM using this approach, giving a work output of $W_{test} = 1.51$ and efficiency $\eta_{test} = 91.0 \%$.

%To ensure comparability between the different cost functions, we continue using the same hyperparameters to train both a bidirectional and a unidirectional LSTM.
%The performance for both networks and two switching times $\Delta \mathrm{T}$ are given in Table \ref{workcosttable}.
%We find that for both $\Delta \mathrm{T}$, the models trained on the extracted work perform worse than their MSE-trained analogues.
%This is not unexpected, as the non-convexity of the extracted work complicates finding an optimal solution.
%We circumvent this problem in the creation of training data for the MSE loss by initialising the optimiser in different states.
%This problem becomes more complex when trying to optimise multiple drive sequence at the same time, as is the case when using the work directly as a loss function.