% !TeX root = ../BA_main_englisch.tex
% !TeX spellcheck = en_GB
In this work we used Long Short-Term Memory networks to predict transducer policies given an excitation driving a two-level quantum system.
The performance was compared to the lower bound derived through local optimisation.
We demonstrated that our approach is capable of predicting transducer policies that will produce a positive work output given a drive data set.
We found that the efficiency greatly depends on the model parameters $N$ and $\Delta \mathrm{T}$.
For $N=2$ and $\rho_0 = \ket{+}\bra{+}$, where an analytic optimum exists, a simple linear network is able to reproduce the optimal policy.
For the higher-dimensional case $N=5$, we examined two switching times $\Delta \mathrm{T}$, finding that the network performs better on the shorter than the longer time.
For $\Delta \mathrm{T} = 5$, the models were unable to outperform the lower bound and were only able to extract 33.1 \% of the optimum.
The reason for the lower performance is likely a combination of two factors.
The models struggle to find a representation of the system state, which is necessary to predict the optimal next step.
Additionally, we found an inherent difference in the optimal solutions between the two switching times: for $\Delta \mathrm{T} = 1$, the transducer Hamiltonian strength was maximised for most samples and thus $\theta_T$ easy to predict.
For $\Delta \mathrm{T} = 5$, where the amount of reachable states on the Bloch sphere is greater, the optimal transducer strength is chosen such that the evolved state is in a favourable position.
This dependence on both the initial and evolved state adds difficulty.
In all cases the bidirectional outperformed the unidirectional LSTM, as the former has access to the complete drive sequence while the latter can only make predictions from the previous and current drive qubits.

We identified the use of the mean squared error as loss a possible disadvantage in training, especially for $\Delta \mathrm{T} = 5$.
For $\Delta \mathrm{T} = 1$ we trained a bidirectional model on the extracted work directly which performed slightly worse than the model trained on MSE loss but has better generalisability.
Overall, the main difficulty in all models seems to have been representing the current system state correctly.
In our approach, this representation had to be found implicitly, as $\rho_S$ was not included in the training data.
An alternative would be following an approach similar to those in \cite{Banchi_2018, PhysRevX.10.011006}, where recurrent neural networks are used to directly model the system dynamics.

In all machine learning based approaches, the performance of such models highly depends on the choice of hyperparameters.
We employed Bayesian optimisation (see Appendix \ref{training} for more details) to find hyperparameters that maximise performance.
However, the approach by limited in the amount of computing power available and the maximum over the total hyperparameter space may not be found in finite time.

It has to be noted that the approach followed in this work depends on maximising expectation values of work outputs.
While this is certainly justified in the limit of infinite experimental realisations, the question of how the results might differ in the single shot case remains.
A reinforcement learning based approach, where problems are usually formulated as Markov Decision Problems and probabilistic reward functions are common \cite{Sutton1998}, would be a suitable alternative.
Additionally, this approach could be extended to the case of continuous drive and transducer functions.

We calculated the work output as the negative expectation value of the internal energy of the system, which is extracted through the use of transducer qubits.
The question of how this output could be accessed experimentally, remains open and requires further investigation.