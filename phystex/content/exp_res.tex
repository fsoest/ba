In the following sections we will apply neural networks to the system outlined before.
The training data is created as in the previous section, sampling random drive sequences $\{|\psi_D^n \rangle\}$ and determining their respective optimal transducer protocols $\{|\psi_T^n \rangle\}$ using the optimiser.
The drive sequences, represented by $\{\theta_D^n, \phi_D^n\}$, are sampled randomly from the Haar measure \cite{Mezzadri}.

The networks are trained to learn the mapping $\{|\psi_D^n \rangle\} \to \{|\psi_T^n \rangle\}$.
Both the input (drive) and output (transducer) are transformed by the embedding
\begin{align} \label{embedding}
	\left\{
	\begin{pmatrix}
	\theta^n & \phi^n \\
	\end{pmatrix}
	\right\}
	\to
	\left\{
	\begin{pmatrix}
	\sin(\theta^n) & \sin(\phi^n) & \cos(\theta^n)  & \cos(\phi^n) \\
	\end{pmatrix}
	\right\}.
\end{align}
The reasons for this operation are twofold: it normalises the data to the interval $[-1, 1]$, which is beneficial to learning \cite{LeCun2012}. Additionally it encodes information regarding the periodicity of the qubit angle representation.
We add either a zero or a one as an extra input parameter for LSTM training depending on whether or not the input is the last in a sequence.

\subsection{Choice of cost function for training}
Throughout most of this work we use the MSE as introduced in Section \ref{trainbackprop}.
It should be noted that using the MSE for training is not the only choice, and perhaps not the obvious one.
Directly using the work extracted is possibly a more intuitive choice, but we refrain from it in the beginning for two main reasons.
Firstly, the interaction Hamiltonian must be known to the experimenter for use as a cost function.
Secondly, the computation of the extracted work becomes costly for larger $N$.

\subsection{Evaluation of network performance}
To compare the accuracy of different models a performance indicator is required. 
Naturally one might use the MSE.
Instead we define the \textit{efficiency} of a model $\textfrak{N}$ on a dataset $\{(\vec{x}_i, \vec{y}_i)\} = \{(\{\ket{\psi_D^n}\}_i, \{\ket{\psi_T^n}\}_i) \}$ as
\begin{align}
	\eta = \frac{1}{N} \sum_{i=1}^N \frac{W(\vec{x}_i,\textfrak{N}(\vec{x}_i))}{W(\vec{x}_i,\vec{y}_i)},
\end{align}
i.e. the arithmetic mean of the ratios of work output predicted by the model to optimal work output.
The function $W(\vec{x}_i, \vec{y}_i) = W(\{\ket{\psi_D^n}\}_i, \{\ket{\psi_T^n}\}_i)$ returns the work given a drive and transducer sequence.

See Appendix \ref{training} for a detailed description of the training implemented as well as the hyperpara-meters used in this work.